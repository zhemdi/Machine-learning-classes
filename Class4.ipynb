{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Introduction to Artificial Intelligence and Machine Learning\n",
    "    * What is Artificial Intelligence (AI)?\n",
    "    * What is Machine Learning (ML)?\n",
    "    * Relationship between AI and ML\n",
    "    * Importance of Machine Learning\n",
    "2. Goals and Objectives of Machine Learning\n",
    "    * Supervised Learning\n",
    "    * Unsupervised Learning\n",
    "    * Reinforcement Learning\n",
    "    * Semi-supervised Learning\n",
    "    * Self-supervised Learning\n",
    "    * Transfer Learning\n",
    "3. Basic Concepts in Machine Learning\n",
    "    * Features and Labels\n",
    "    * Training and Testing Data\n",
    "    * Model Evaluation Metrics (Accuracy, Precision, Recall, F1-Score)\n",
    "4. Main Directions in Machine Learning\n",
    "    * Regression\n",
    "    * Classification\n",
    "    * Clustering\n",
    "    * Dimensionality Reduction\n",
    "    * Anomaly Detection\n",
    "5. Basic Machine Learning Methods\n",
    "    * Linear Regression\n",
    "    * Logistic Regression\n",
    "    * Decision Trees\n",
    "    * k-Nearest Neighbors\n",
    "    * Support Vector Machines\n",
    "6. Introduction to Deep Learning\n",
    "    * Artificial Neural Networks\n",
    "    * Convolutional Neural Networks\n",
    "    * Recurrent Neural Networks\n",
    "    * Generative Adversarial Networks\n",
    "    * Transfer Learning\n",
    "7. Python Libraries for Machine Learning\n",
    "    * NumPy\n",
    "    * pandas\n",
    "    * Matplotlib\n",
    "    * scikit-learn\n",
    "    * TensorFlow\n",
    "    * Keras\n",
    "    * PyTorch\n",
    "8. Machine Learning Examples\n",
    "    * Simple Linear Regression Example\n",
    "    * Image Classification Example\n",
    "    * Time Series Forecasting Example\n",
    "9. Conclusion\n",
    "    * Recap of the tutorial\n",
    "    * Real-world applications of Machine Learning\n",
    "    * Further resources and recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Artificial Intelligence and Machine Learning\n",
    "### 2.1. What is Artificial Intelligence (AI)?\n",
    "Artificial Intelligence (AI) is the field of computer science that aims to create machines that can simulate or mimic human intelligence. The goal is to enable computers to perform tasks that would typically require human intelligence, such as speech recognition, visual perception, decision-making, and natural language understanding.\n",
    "<img src=\"https://www.researchgate.net/profile/Stephan-De-Spiegeleire/publication/316983844/figure/fig5/AS:494820222095361@1494985742395/AN-OVerVieW-OF-NOTABLe-APPrOACHeS-AND-DiSCiPLiNeS-iN-Ai-AND-MACHiNe-LeArNiNg-101_W640.jpg\" alt=\"AI Overview\" width=\"600\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. What is Machine Learning (ML)?\n",
    "Machine Learning (ML) is a subset of Artificial Intelligence that involves the development of algorithms that enable computers to learn from and make predictions or decisions based on data. Rather than being explicitly programmed, these algorithms allow computers to improve their performance on a specific task as they are exposed to more data over time.\n",
    "<img src=\"https://miro.medium.com/max/1000/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" alt=\"ML Overview\" width=\"600\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Relationship between AI and ML\n",
    "AI is a broad field that encompasses various techniques and approaches to achieve intelligent behavior in machines. Machine Learning is one of the approaches within AI, and it's currently the most successful and widely used. In other words, Machine Learning is a means to achieve AI.\n",
    "<img src=\"https://i0.wp.com/blog.forumias.com/wp-content/uploads/2022/07/AI-ML-DL.jpg?w=958&ssl=1\" alt=\"AI and ML Relationship\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Importance of Machine Learning\n",
    "Machine Learning has become increasingly important in recent years due to its ability to solve complex problems and provide valuable insights from vast amounts of data. Some key factors driving the growth of Machine Learning include:\n",
    "1. **Data Availability**: The exponential growth of data generated by various sources, such as social media, IoT devices, and e-commerce, has provided a rich source of information for Machine Learning algorithms to learn from.\n",
    "2. **Computational Power**: Advances in computing hardware, such as GPUs and TPUs, have enabled the processing of large datasets and the training of complex models in a much shorter time.\n",
    "3. **Algorithmic Advancements**: Researchers and practitioners have developed more efficient and accurate Machine Learning algorithms, enabling better performance across a wide range of tasks.\n",
    "4. **Real-world Applications**: Machine Learning has been successfully applied to various domains, including healthcare, finance, transportation, and marketing, leading to significant improvements in efficiency, accuracy, and decision-making.\n",
    "![Importance of ML](https://miro.medium.com/max/1200/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Goals and Objectives of Machine Learning\n",
    "Machine learning is a subfield of artificial intelligence that focuses on creating algorithms that can learn from and make predictions or decisions based on data. The primary goal of machine learning is to enable machines to automatically learn from past experiences, adapt to new situations, and improve their performance without explicit programming. There are several different approaches to achieving this goal, and these can be broadly categorized into five types: Supervised Learning, Unsupervised Learning, Reinforcement Learning, Semi-supervised Learning, and Transfer Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Supervised Learning\n",
    "![Supervised Learning](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Supervised-Learning-in-ML.jpg)\n",
    "\n",
    "Supervised Learning is the most common type of machine learning. In this approach, an algorithm is trained on a labeled dataset, where the input features and their corresponding output labels are provided. The primary goal of supervised learning is to learn a mapping from inputs to outputs and use this mapping to make predictions on new, unseen data. Supervised learning can be further divided into two types: classification and regression. In classification, the output is a discrete label, while in regression, the output is a continuous value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Unsupervised Learning\n",
    "![Unsupervised Learning](https://miro.medium.com/v2/resize:fit:1400/0*tamvSiqDneDfw2Vr)\n",
    "\n",
    "Unsupervised Learning is a type of machine learning where the algorithm is provided with an unlabeled dataset, and its objective is to find patterns or structures in the data. Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection. In clustering, the goal is to group similar data points together, while in dimensionality reduction, the goal is to reduce the number of features in the dataset while preserving its underlying structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Reinforcement Learning\n",
    "![Reinforcement Learning](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4u2GtNnMa9xso1WkLh7hVA.png)\n",
    "\n",
    "Reinforcement Learning is an approach to machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties for its actions and adjusts its behavior accordingly to maximize the cumulative reward. Reinforcement learning is particularly useful in situations where the optimal solution is not known in advance and must be learned through trial and error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Semi-supervised Learning\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzVENZj3bWrwhRBN4hQq7Q.png\" alt=\"Semi-supervised Learning\" width=\"600\"/>\n",
    "\n",
    "Semi-supervised Learning is a type of machine learning that combines elements of both supervised and unsupervised learning. In this approach, the algorithm is trained on a dataset that contains both labeled and unlabeled data. The goal of semi-supervised learning is to leverage the information contained in the unlabeled data to improve the performance of the algorithm on the labeled data. This can be particularly useful in situations where obtaining labeled data is expensive or time-consuming.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Self-supervised Learning\n",
    "<img src=\"https://velog.velcdn.com/images/jaeheon-lee/post/b7c932cc-cc96-4b1d-a3a9-d890e8ba3d3c/image.png\" alt=\"Self-supervised Learning\" width=\"600\"/>\n",
    "Self-supervised Learning is an emerging type of machine learning that aims to learn useful representations of data without relying on human-annotated labels. It can be considered as a subcategory of unsupervised learning but with a specific focus on learning representations that can be useful for downstream tasks, such as classification or regression.\n",
    "\n",
    "In self-supervised learning, an algorithm learns to solve a \"pretext\" task, which is derived from the input data itself, without requiring any additional labels. For example, the pretext task could be predicting the next word in a sentence, predicting the rotation of an image, or predicting the color of a grayscale image. By solving these pretext tasks, the algorithm learns to extract meaningful features from the data that can be later used for other tasks.\n",
    "\n",
    "One of the main advantages of self-supervised learning is that it can leverage large amounts of unlabeled data to learn useful representations, which can then be fine-tuned on smaller labeled datasets for specific tasks. This approach has been particularly successful in the field of natural language processing and computer vision, where self-supervised learning has led to significant improvements in performance on a variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of self-supervised learning using image rotation as the pretext task\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the rotation transforms\n",
    "rotate_transforms = [\n",
    "    transforms.RandomRotation(degrees=(0, 0)),\n",
    "    transforms.RandomRotation(degrees=(90, 90)),\n",
    "    transforms.RandomRotation(degrees=(180, 180)),\n",
    "    transforms.RandomRotation(degrees=(270, 270))\n",
    "]\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "dataset = CIFAR10(root=\"./data\", train=True, transform=transforms.Compose([\n",
    "    transforms.RandomChoice(rotate_transforms),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# In the training loop, use the rotation angle as the pretext task label\n",
    "for images, _ in dataloader:\n",
    "    # Train the self-supervised model using the rotated images and the corresponding rotation angles\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the self-supervised learning task is to predict the rotation angle applied to each image in the CIFAR-10 dataset. By learning to solve this task, the model will learn useful features that can be later used for other tasks, such as image classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Transfer Learning\n",
    "![Transfer Learning](https://www.mdpi.com/sensors/sensors-23-00570/article_deploy/html/images/sensors-23-00570-g001-550.jpg)\n",
    "\n",
    "Transfer Learning is an approach to machine learning where a pre-trained model is fine-tuned on a new dataset. The idea behind transfer learning is that the knowledge gained from solving one problem can be applied to solve a related problem more efficiently. This is particularly useful in deep learning, where training a model from scratch can be computationally expensive and time-consuming. By using a pre-trained model as a starting point, the training process can be significantly accelerated, and better performance can be achieved with less data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Concepts in Machine Learning\n",
    "### 3.1. Features and Labels\n",
    "<img src=\"https://majdarbash.github.io/assets/images/aws-cmls/observations-features-labels.png\" alt=\"Features and Labels\" width=\"600\"/>\n",
    " \n",
    "\n",
    "In machine learning, the data we work with is usually represented in a structured format, with a set of attributes known as features, and a target attribute known as the label. Features are the variables that describe the data, while the label is the output variable that we want to predict or classify.\n",
    "\n",
    "For example, in an email spam detection problem, features could be the words or phrases in the email, the sender's email address, and the time the email was sent. The label would be a binary variable indicating whether the email is spam or not spam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training and Testing Data\n",
    " \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Traintest.svg/2880px-Traintest.svg.png\" alt=\"Training and Testing Data\" width=\"600\"/>\n",
    "\n",
    "In order to build a machine learning model, we need to split our dataset into two separate parts: the training data and the testing data. The training data is used to train the model and learn the relationship between the features and the label. The testing data is used to evaluate the model's performance on unseen data and to ensure that the model is not overfitting to the training data.\n",
    "\n",
    "Typically, the dataset is split into 70-80% training data and 20-30% testing data. It is important to make sure that both the training and testing data have a similar distribution of the label variable to ensure that the model performs well on both sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model Evaluation Metrics\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*1WPbfzztdv50V22TpA6njw.png\" alt=\"Model Evaluation Metrics\" width=\"600\"/>\n",
    "\n",
    "Model evaluation metrics are used to measure the performance of a machine learning model. There are several evaluation metrics available, and the choice of which one to use depends on the problem at hand. Some of the most common evaluation metrics are:\n",
    "1. **Accuracy**: The proportion of correct predictions out of the total predictions made. It is suitable for balanced datasets, but not for imbalanced datasets where one class is significantly more frequent than the other(s).\n",
    "2. **Precision**: The proportion of true positive predictions out of the total positive predictions made. Precision is a measure of how well the model correctly identifies positive instances.\n",
    "3. **Recall**: The proportion of true positive predictions out of the total actual positive instances. Recall is a measure of how well the model identifies all the positive instances.\n",
    "4. **F1-Score**: The harmonic mean of precision and recall. F1-score is a balanced metric that considers both precision and recall, making it suitable for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Example of calculating evaluation metrics for a classification problem\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [1, 1, 1, 0, 0, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we calculate the accuracy, precision, recall, and F1-score for a binary classification problem using the '**scikit-learn**'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Main Directions in Machine Learning\n",
    "### 4.1.Regression\n",
    " \n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-in-machine-learning.png\" alt=\"Regression\" width=\"600\"/>\n",
    "\n",
    "Regression is a type of supervised learning task where the goal is to predict a continuous output variable based on the input features. In regression, the relationship between the input features and the output variable is modeled using a mathematical function. Common examples of regression tasks include predicting house prices, stock prices, and customer lifetime value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset (e.g., Boston Housing dataset)\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "data['PRICE'] = boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data.drop('PRICE', axis=1)\n",
    "y = data['PRICE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "Linear Regression is a supervised machine learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes that there's a linear relationship between the input features and the target variable. The goal of the algorithm is to find the best-fitting line (in the case of simple linear regression) or plane/hyperplane (in the case of multiple linear regression) that minimizes the sum of the squared differences between the actual target values and the predicted values.\n",
    "![Linear Regression](https://miro.medium.com/max/1280/1*fX95txC9xSwSPeP6ch2nmg.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 \n",
    "Try using a different regression algorithm and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Look into Ridge, Lasso, or ElasticNet from sklearn.linear_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Classification\n",
    " \n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/classification-algorithm-in-machine-learning.png\" alt=\"Classification\" width=\"400\"/>\n",
    "\n",
    "Classification is another type of supervised learning task, where the goal is to predict a discrete output variable (class label) based on the input features. In classification, the relationship between the input features and the output variable is modeled using a function that maps the input features to one of the possible class labels. Common examples of classification tasks include email spam detection, image recognition, and medical diagnosis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Neighbors Classification\n",
    "K Neighbors Classification is a supervised machine learning algorithm used for classification tasks. It is a non-parametric, lazy learning algorithm, meaning that there is no explicit training phase, and the algorithm makes predictions based on the training data itself. The idea behind K Neighbors Classification is to predict the class of a new data point by looking at the K closest data points in the training set and assigning the most common class among these neighbors.\n",
    "![K Neighbors Classification](https://miro.medium.com/max/900/1*OyYyr9qY-w8RkaRh2TKo0w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['Species'] = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data.drop('Species', axis=1)\n",
    "y = data['Species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2  \n",
    "Try using a different classification algorithm and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Look into LogisticRegression, DecisionTreeClassifier, or RandomForestClassifier from sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Clustering\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/merge3cluster.jpg\" alt=\"Clustering\" width=\"600\"/>\n",
    "\n",
    "Clustering is an unsupervised learning task where the goal is to group similar instances together based on their features. Clustering algorithms analyze the input features and identify patterns or structures within the data. The resulting groups, or clusters, are formed such that instances within the same cluster are more similar to each other than to instances in other clusters. Common examples of clustering tasks include customer segmentation, image segmentation, and anomaly detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means\n",
    "K Means is an unsupervised machine learning algorithm used for clustering tasks. The goal of the algorithm is to partition the input data into K distinct clusters based on similarity (usually Euclidean distance). The algorithm works iteratively to assign each data point to one of K clusters based on the features that are provided. Data points are clustered based on feature similarity.\n",
    "![K Means](https://miro.medium.com/max/1400/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Train the model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(data['sepal length (cm)'], data['sepal width (cm)'], c=kmeans.labels_, cmap='viridis')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3  \n",
    "Try using a different clustering algorithm and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Look into DBSCAN, AgglomerativeClustering, or SpectralClustering from sklearn.cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Dimensionality Reduction\n",
    " \n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/Dimensionality_Reduction_1.jpg\" alt=\"Dimensionality Reduction\" width=\"600\"/>\n",
    "\n",
    "Dimensionality reduction is another unsupervised learning task, where the goal is to reduce the number of input features while retaining the most important information in the data. Dimensionality reduction techniques transform the original high-dimensional data into a lower-dimensional representation that is easier to analyze and visualize. This can help improve the performance of machine learning models and reduce the computational cost. Common examples of dimensionality reduction tasks include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "PCA is an unsupervised machine learning algorithm used for dimensionality reduction. It is a linear transformation technique that aims to project the original data into a lower-dimensional subspace while retaining as much of the data's variance as possible. This is achieved by identifying the directions (called principal components) in the feature space along which the variance is maximized.\n",
    "![PCA](https://miro.medium.com/max/874/1*H38t3YUv_QktLwalzDYRRg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Apply PCA\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=iris.target, cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA Dimensionality Reduction')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 4 \n",
    "Try using a different dimensionality reduction algorithm and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Look into t-SNE, UMAP, or LinearDiscriminantAnalysis from sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Anomaly Detection\n",
    " \n",
    "<img src=\"https://dezyre.gumlet.io/images/blog/anomaly-detection-using-machine-learning-in-python-with-example/image_22571226771643385810847.png?w=900&dpr=2.0\" alt=\"Anomaly Detection\" width=\"600\"/>\n",
    "\n",
    "Anomaly detection is a type of machine learning task where the goal is to identify instances that deviate significantly from the norm or the majority of the data. These instances, or anomalies, can represent errors, fraud, or other interesting events that warrant further investigation. Anomaly detection algorithms analyze the input features and assign a score to each instance based on how similar or dissimilar it is to the rest of the data. Common examples of anomaly detection tasks include fraud detection, network intrusion detection, and equipment failure prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elliptic Envelope\n",
    "Elliptic Envelope is an unsupervised machine learning algorithm used for anomaly detection. The algorithm assumes that the data is normally distributed and works by fitting an ellipse (in two dimensions) or an ellipsoid (in higher dimensions) around the central data points. Data points that lie outside the ellipse or ellipsoid are considered outliers or anomalies. The method is sensitive to the assumption of normality, but it is robust in detecting outliers in multivariate data.\n",
    "\n",
    "![Elliptic Envelope](https://miro.medium.com/v2/resize:fit:952/format:webp/1*CYgTfGIjb35GkZCh5NgD4g.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=300)\n",
    "\n",
    "# Train the model\n",
    "outlier_detector = EllipticEnvelope(contamination=0.1)\n",
    "outlier_detector.fit(data)\n",
    "\n",
    "# Predict outliers\n",
    "outliers = outlier_detector.predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(data[:, 0], data[:, 1], c=outliers, cmap='viridis', marker='o')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 \n",
    "Try using a different anomaly detection algorithm and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Look into IsolationForest, LocalOutlierFactor, or OneClassSVM from sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Machine Learning Methods\n",
    "### 5.1. Linear Regression\n",
    "Linear Regression is a simple machine learning algorithm that models the linear relationship between a dependent variable (output) and one or more independent variables (features). It is mainly used for regression tasks, i.e., predicting continuous values.\n",
    "![Linear Regression](https://miro.medium.com/max/1280/1*fX95txC9xSwSPeP6ch2nmg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "Try using different independent variables for the Linear Regression model and see how it affects the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# 1. Select different columns from the dataset as independent variables.\n",
    "# 2. Train and evaluate the Linear Regression model as shown in the example above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Logistic Regression\n",
    "Logistic Regression is a variation of Linear Regression that is used for binary classification tasks. It models the probability of the default class using the logistic function, which outputs a value between 0 and 1. Logistic Regression can also be extended to multi-class classification using techniques like one-vs-rest or multinomial logistic regression.\n",
    "<img src=\"https://miro.medium.com/max/1400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" alt=\"Logistic Regression\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "Try using different independent variables for the Logistic Regression model and see how it affects the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# 1. Select different columns from the dataset as independent variables.\n",
    "# 2. Train and evaluate the Logistic Regression model as shown in the example above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Decision Trees\n",
    "Decision Trees are a class of machine learning algorithms that are used for both regression and classification tasks. They work by recursively splitting the input space into regions based on the values of the input features. The tree is grown until a stopping criterion is reached, such as a maximum tree depth or minimum number of samples per leaf.\n",
    "\n",
    "![Decision Trees](https://miro.medium.com/max/1400/1*XMId5sJqPtm8-RIwVVz2tg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "Try using different hyperparameters for the Decision Tree model, such as the maximum depth or minimum samples split, and see how it affects the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# 1. Adjust the hyperparameters when initializing the DecisionTreeClassifier.\n",
    "# 2. Train and evaluate the Decision Tree model as shown in the example above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. k-Nearest Neighbors\n",
    "k-Nearest Neighbors (k-NN) is a simple machine learning algorithm used for classification and regression tasks. It works by finding the k training examples that are closest to a new input instance and predicting the output based on the majority class (for classification) or the average value (for regression) of these neighbors.\n",
    "\n",
    "![k-Nearest Neighbors](https://miro.medium.com/max/1000/1*OyYyr9qY-w8RkaRh2TKo0w.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9 \n",
    "Try using different values of k and different distance metrics for the k-NN model and see how it affects the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# 1. Adjust the value of n_neighbors and the distance metric when initializing the KNeighborsClassifier.\n",
    "# 2. Train and evaluate the k-NN model as shown in the example above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Support Vector Machines\n",
    "Support Vector Machines (SVM) are a class of machine learning algorithms used for classification and regression tasks. They work by finding the hyperplane that best separates the classes (for classification) or predicts the target values (for regression) while maximizing the margin between the hyperplane and the nearest data points (support vectors).\n",
    "\n",
    "\n",
    "<img src=\"https://torchbearer.readthedocs.io/en/0.3.0/_images/svm_fit.gif\" alt=\"Support Vector Machines\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Support Vector Machines model\n",
    "svm = SVC()\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10 \n",
    "Try using different kernel functions and regularization parameters for the SVM model and see how it affects the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# 1. Adjust the kernel function and regularization parameter when initializing the SVC.\n",
    "# 2. Train and evaluate the SVM model as shown in the example above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Introduction to Deep Learning\n",
    "Deep learning is a subset of machine learning that focuses on neural network architectures with many layers, also known as deep neural networks. These deep neural networks can automatically learn to represent data by training on large datasets. In this section, we will discuss various deep learning architectures and their applications.\n",
    "\n",
    "### 6.1. Artificial Neural Networks\n",
    "Artificial Neural Networks (ANNs) are inspired by the human brain and consist of interconnected artificial neurons. ANNs are designed to recognize patterns and make decisions by learning from input data.\n",
    "\n",
    "![Artificial Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png)\n",
    "\n",
    "\n",
    "Here is a simple implementation of a feedforward neural network using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have loaded and preprocessed the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    "Modify the architecture of the neural network by adding more hidden layers or changing the number of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add more layers to the model by adding more `layers.Dense()` instances\n",
    "# For example, you can add another hidden layer with 128 neurons like this:\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Convolutional Neural Networks\n",
    "Convolutional Neural Networks (CNNs) are a type of deep learning architecture designed specifically for processing grid-like data, such as images. CNNs consist of convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "![Convolutional Neural Network](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n",
    "\n",
    "\n",
    "Here's a simple implementation of a CNN using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have loaded and preprocessed the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11 \n",
    "Try changing the number of filters in the convolutional layers or modify the kernel size to see how it affects the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the number of filters in the convolutional layers by changing the first argument of `layers.Conv2D()`\n",
    "# For example, you can change the first convolutional layer to have 64 filters like this:\n",
    "\n",
    "layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1))\n",
    "\n",
    "# You can also change the kernel size by modifying the second argument of `layers.Conv2D()`\n",
    "# For example, you can change the kernel size of the first convolutional layer to (5, 5) like this:\n",
    "\n",
    "layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network architecture designed for processing sequences of data. RNNs contain loops that allow information to persist across time steps, making them well-suited for time series data or natural language processing tasks.\n",
    "\n",
    "![Recurrent Neural Network](https://miro.medium.com/max/1838/1*SKGAqkVVzT6co-sZ29ze-g.png)\n",
    "\n",
    "![Recurrent Neural Network](https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
    "\n",
    "\n",
    "Here's a simple implementation of an RNN using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.SimpleRNN(64, activation='relu', input_shape=(None, 28)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have loaded and preprocessed the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "Modify the RNN architecture by adding more recurrent layers or changing the number of units in the recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add more recurrent layers to the model by adding more `layers.SimpleRNN()` instances\n",
    "# For example, you can add another SimpleRNN layer with 128 units like this:\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.SimpleRNN(64, activation='relu', input_shape=(None, 28), return_sequences=True),\n",
    "    layers.SimpleRNN(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Generative Adversarial Networks\n",
    "Generative Adversarial Networks (GANs) are a type of deep learning architecture that consists of two neural networks, a generator, and a discriminator. The generator learns to create realistic samples, while the discriminator learns to distinguish between real and generated samples. The two networks are trained in a competitive setting, where the generator tries to fool the discriminator and the discriminator tries to correctly identify the samples.\n",
    "\n",
    "![Generative Adversarial Network](https://i.imgur.com/6NMdO9u.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Transfer Learning\n",
    "Transfer learning is a technique in deep learning where a pre-trained model is used as a starting point for training a new model. By leveraging the knowledge from the pre-trained model, transfer learning can lead to faster training times and improved performance, especially when dealing with small datasets.\n",
    "\n",
    "![Transfer Learning](https://www.bbvaaifactory.com/media/post_transfer_learning/gif_transferlearning_prediction_v3.gif)\n",
    "\n",
    "\n",
    "Here's an example of using transfer learning with TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have loaded and preprocessed the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13 \n",
    "Try using a different pre-trained model, such as '**ResNet50**', '**InceptionV3**', or '**MobileNet**', and see how it affects the performance of your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a different pre-trained model, import the corresponding module and instantiate it as the base model\n",
    "# For example, to use ResNet50, do the following:\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Python Libraries for Machine Learning\n",
    "### 7.1. NumPy\n",
    "NumPy is an open-source library in Python that provides support for arrays and matrices, along with a large collection of high-level mathematical functions. It is an essential library for machine learning as it provides efficient numerical computations and a wide range of mathematical operations that are commonly used in machine learning algorithms.\n",
    "\n",
    "\n",
    "**Example: Creating a NumPy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14 \n",
    "Create a 3x3 NumPy array containing random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# - Use np.random.rand() function\n",
    "\n",
    "# Code sketch\n",
    "import numpy as np\n",
    "random_arr = # your code here\n",
    "print(random_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  7.2. pandas\n",
    "pandas is an open-source library that provides data manipulation and data analysis tools in Python. It offers data structures such as Series and DataFrame, which are built on top of NumPy arrays. pandas makes it easy to load, manipulate, and analyze data in a tabular format, which is very important in the data preprocessing stage of machine learning.\n",
    "\n",
    "**Example: Creating a pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, 3],\n",
    "        'B': [4, 5, 6],\n",
    "        'C': [7, 8, 9]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15\n",
    "Load the Iris dataset from a CSV file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# - Use pd.read_csv() function\n",
    "# - Iris dataset URL: 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Code sketch\n",
    "import pandas as pd\n",
    "url = # your code here\n",
    "iris_df = # your code here\n",
    "print(iris_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Matplotlib\n",
    "Matplotlib is a widely used Python library for creating static, animated, and interactive visualizations. It provides a MATLAB-like interface for creating plots, histograms, bar charts, scatter plots, and more. Visualizing data is an important step in understanding the underlying patterns and relationships in the data, which helps in building better machine learning models.\n",
    "\n",
    "**Example: Plotting a simple line graph using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.title('Line graph')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16 \n",
    "Create a bar chart to visualize the distribution of a categorical variable in the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# - Use plt.bar() function\n",
    "\n",
    "# Code sketch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris_df = pd.read_csv(url, header=None)\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = # your code here\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(# your code here)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. scikit-learn\n",
    "scikit-learn is a popular open-source library in Python for machine learning, providing a wide range of algorithms for classification, regression, clustering, and dimensionality reduction. It also provides tools for model evaluation, parameter tuning, and preprocessing. scikit-learn is built on top of NumPy, SciPy, and matplotlib, making it an essential library for machine learning in Python.\n",
    "\n",
    "**Example: Training a simple linear regression model using scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17 \n",
    "Train a decision tree classifier on the Iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# - Use DecisionTreeClassifier from sklearn.tree\n",
    "# - Use train_test_split() function\n",
    "# - Use accuracy_score() function from sklearn.metrics\n",
    "\n",
    "# Code sketch\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris_df = pd.read_csv(url, header=None)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = iris_df.iloc[:, :-1].values\n",
    "y = iris_df.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. TensorFlow\n",
    "TensorFlow is an open-source machine learning library developed by Google. It provides a flexible platform for defining and running machine learning algorithms, with support for deep learning and other advanced techniques. TensorFlow is particularly suited for building large-scale neural networks and deploying them on various hardware platforms, such as CPUs, GPUs, and TPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Keras\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, or Theano. It was developed with a focus on enabling fast experimentation, providing an easy-to-use interface for building and training deep learning models. Keras has been integrated into TensorFlow as its official high-level API since TensorFlow 2.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 PyTorch\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab, which provides tensor computation and deep neural networks built on tape-based autograd system. It is widely used for applications such as natural language processing, computer vision, and reinforcement learning. PyTorch is known for its dynamic computational graph, which allows for easier debugging and more flexibility in building complex models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Learning Examples\n",
    "### 8.1. Simple Linear Regression Example\n",
    "In this example, we will use a simple linear regression model to predict the relationship between two variables. We will generate synthetic data and split it into training and testing sets. Then, we will train a linear regression model using scikit-learn and evaluate its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Visualize the regression line\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, 2 * X + 1, color='red', linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression Example')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 18\n",
    "Modify the above code to use a different dataset, and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints:\n",
    "# - Create your own dataset, or use an existing one\n",
    "# - Ensure the dataset has only one input feature\n",
    "\n",
    "# Code sketch\n",
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Image Classification Example\n",
    "In this example, we will use the famous MNIST dataset, which consists of 70,000 handwritten digits, to train a  classifier for image classification.\n",
    "\n",
    "Look at tutorial [Class4Extra](https://colab.research.google.com/drive/1R6VS3TdYDitCYi4MF7APtXjrz-v69dnK?usp=sharing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Time Series Forecasting Example\n",
    "In this example, we will use the famous Airline Passengers dataset, which shows the monthly number of airline passengers between 1949 and 1960, to train a simple time series forecasting model using the ARIMA method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
