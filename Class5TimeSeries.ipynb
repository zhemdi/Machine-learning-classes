{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series: classical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Time Series\n",
    "* Definition of time series\n",
    "* Importance and applications of time series analysis\n",
    "* Types of time series data (univariate, multivariate)\n",
    "* Time series components (trend, seasonality, cyclicity, irregularity)\n",
    "### 2. Exploratory Data Analysis for Time Series\n",
    "* Importing time series data into Python\n",
    "* Visualizing time series data (line plots, seasonal plots)\n",
    "* Handling missing values and outliers\n",
    "* Stationarity testing (Augmented Dickey-Fuller test)\n",
    "* Decomposing time series components\n",
    "### 3. Traditional Time Series Forecasting Methods\n",
    "* Simple methods (Naive, Average, Drift)\n",
    "* Exponential Smoothing (Simple, Holt's, Holt-Winters)\n",
    "* Autoregressive Integrated Moving Average (ARIMA) models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Time Series\n",
    "**Definition of Time Series**\n",
    "\n",
    "A time series is a collection of data points indexed in chronological order, typically with uniform time intervals. It represents a sequence of observations or measurements taken at successive points in time, often capturing the evolution of a particular phenomenon over a period.\n",
    "\n",
    "**Importance and Applications of Time Series Analysis**\n",
    "\n",
    "Time series analysis is crucial in various fields, including finance, economics, meteorology, signal processing, and many others. It enables us to:\n",
    "\n",
    "* Understand patterns and trends in data over time\n",
    "* Identify seasonality, cyclicity, and other recurring patterns\n",
    "* Forecast future values based on historical data\n",
    "* Detect anomalies or unusual behavior\n",
    "* Make informed decisions and predictions\n",
    "**Types of Time Series Data**\n",
    "1. **Univariate Time Series**: This type of time series consists of a single variable observed over time, such as stock prices, temperature readings, or sales figures.\n",
    "2. **Multivariate Time Series**: This involves multiple variables observed over the same time period, such as various economic indicators or sensor measurements from different sources.\n",
    "\n",
    "**Time Series Components**\n",
    "Time series data can often be decomposed into the following components:\n",
    "1. **Trend**: The underlying long-term pattern or movement, representing the overall direction of the data (increasing, decreasing, or stable).\n",
    "2. **Seasonality**: Periodic fluctuations or patterns that repeat over fixed intervals of time, such as daily, weekly, monthly, or yearly cycles.\n",
    "3. **Cyclicity**: Longer-term fluctuations or cycles that are not strictly periodic, often driven by economic or business cycles.\n",
    "4. **Irregularity (Residual)**: The remaining random or irregular variations in the data that cannot be explained by the other components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis for Time Series\n",
    "### **Importing Time Series Data into Python**\n",
    "\n",
    "Python provides several libraries for working with time series data, such as Pandas, NumPy, and datetime. Here's an example of importing a time series dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://github.com/jbrownlee/Datasets/raw/master/airline-passengers.csv'\n",
    "\n",
    "# Load data from CSV\n",
    "airline_data = pd.read_csv(url, index_col='Month', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**:\n",
    "Load the Daily Minimum Temperatures dataset. Replace the blanks with the correct information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "* Replace the first blank with the dataset URL https://raw.githubusercontent.com/upul/WhiteBoard/master/data/daily-minimum-temperatures-in-me.csv.\n",
    "* Fill in the second blank with the index column name, which is **'Date'**.\n",
    "* Set the number of rows to nrows=3650.\n",
    "* Rename the temperature column to **'Temperature'**.\n",
    "* Clean column **'Temperature'** by removing **'?'** and converting it to float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_min_temp_url = 'https://raw.githubusercontent.com/upul/WhiteBoard/master/data/daily-minimum-temperatures-in-me.csv'\n",
    "\n",
    "nrows = 3650\n",
    "# Load the Daily Minimum Temperatures dataset\n",
    "temp_data = pd.read_csv(, nrows=nrows, index_col=, parse_dates=True)\n",
    "print(temp_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns  to ['Temperature']\n",
    "temp_data.columns = \n",
    "# Remove '?' from the Temperature column: if a string contains a '?', replace it to ''\n",
    "temp_data['Temperature'] = temp_data['Temperature'].str.replace(, )\n",
    "# Convert column Daily Minimum Temperatures to float using astype(float)\n",
    "temp_data['Temperature'] = \n",
    "# Print the first 5 rows of the dataset using head()\n",
    "temp_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing Time Series Data**\n",
    "\n",
    "Visualizing time series data is essential for understanding patterns and trends. Common visualization techniques include:\n",
    "#### 1. **Line Plots**: Plotting the data points over time to see the overall trend and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Line plot for Airline Passengers\n",
    "airline_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**:\n",
    "Create a line plot for the Daily Minimum Temperatures dataset.\n",
    "**Instructions**:\n",
    "* Replace the blank with plot() to generate the line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot for Daily Minimum Temperatures\n",
    "temp_data.____()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Seasonal Plots**: Visualizing the seasonal patterns by plotting the data over a specific time period (e.g., months or days of the week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal plot (monthly)\n",
    "airline_data.groupby(airline_data.index.month).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:\n",
    "Create a seasonal plot for the Daily Minimum Temperatures dataset.\n",
    "**Instructions**:\n",
    "* Replace the blanks with the correct information to generate the seasonal plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.groupby().mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Missing Values and Outliers**\n",
    "\n",
    "Missing values and outliers can significantly impact time series analysis. Python libraries like Pandas provide methods for handling missing data, such as interpolation or filling with specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values\n",
    "airline_data.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**:\n",
    "Handle missing values in the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blank with the correct method to fill missing values using linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values in the temperature dataset\n",
    "temp_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection and removal can be performed using statistical techniques or domain-specific knowledge.\n",
    "\n",
    "### **Stationarity Testing**\n",
    "\n",
    "Stationarity is an important concept in time series analysis, as many forecasting models assume that the data is stationary (i.e., its statistical properties do not change over time). A time series is considered stationary if its statistical properties, such as mean, variance, and autocorrelation, are constant over time. In simpler terms, a stationary series does not exhibit trend or seasonal effects and has constant long-term behavior. This is essential for developing reliable predictive models, as the stationarity assumption allows for consistency in the model's parameters over time.\n",
    "\n",
    "#### The ADF Test Explained\n",
    "\n",
    "The ADF test checks for the presence of a unit root in a time series, which would indicate non-stationarity. It is an extension of the Dickey-Fuller test and provides a more robust approach by accommodating higher order autoregressive processes.\n",
    "\n",
    "The test is performed by estimating the following regression:\n",
    "\n",
    "$$ \\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\cdots + \\delta_{p-1} \\Delta y_{t-p+1} + \\varepsilon_t $$\n",
    "\n",
    "- $ y_t $ is the time series.\n",
    "- $ \\Delta y_t $ is the first difference of the series (i.e., $ y_t - y_{t-1} $).\n",
    "- $ \\alpha $ is a constant term.\n",
    "- $ \\beta t $ represents a deterministic trend.\n",
    "- $ \\gamma $ is the coefficient on $ y_{t-1} $, which is the key to the test. The null hypothesis is $ \\gamma = 0 $, indicating a unit root (non-stationary). The alternative hypothesis is $ \\gamma < 0 $, indicating stationarity.\n",
    "- $ \\delta_1, \\cdots, \\delta_{p-1} $ are the coefficients of the lagged difference terms.\n",
    "- $ \\varepsilon_t $ is the error term.\n",
    "\n",
    "#### Hypothesis in the ADF Test\n",
    "\n",
    "- Null Hypothesis ($ H_0 $): The series has a unit root (non-stationary).\n",
    "- Alternative Hypothesis ($ H_1 $): The series does not have a unit root (stationary).\n",
    "\n",
    "#### Interpreting the ADF Test Result\n",
    "\n",
    "- **ADF Statistic**: A negative value is more supportive of stationarity. The more negative the statistic, the stronger the rejection of the null hypothesis.\n",
    "- **p-value**: If the p-value is less than a significance level (commonly 0.05), the null hypothesis is rejected, suggesting the series is stationary.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "The ADF test can be performed in Python using the `adfuller` method from the `statsmodels` library. The function returns several outputs, including the ADF statistic and the p-value, which are used to determine stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# ADF test\n",
    "result = adfuller(airline_data['Passengers'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**:\n",
    "Perform the Augmented Dickey-Fuller test on the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blanks with the correct method to perform the ADF test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test\n",
    "temp_result = \n",
    "print(f'ADF Statistic: {temp_result[0]}')\n",
    "print(f'p-value: {temp_result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decomposing Time Series Components**\n",
    "\n",
    "Time series decomposition involves separating the data into its trend, seasonal, and residual components. This can be accomplished using techniques like moving averages or additive/multiplicative decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Additive decomposition\n",
    "result = seasonal_decompose(airline_data, model='additive')\n",
    "\n",
    "# Plot components\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**:\n",
    "Decompose the Daily Minimum Temperatures dataset into its components.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blank with the correct method to decompose the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive decomposition\n",
    "temp_result = seasonal_decompose(temp_data, model='additive', period=)\n",
    "\n",
    "# Plot components\n",
    "temp_result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Time Series Forecasting Methods\n",
    "**Simple Methods**\n",
    "\n",
    "1. **Naive Method**: This method assumes that the future value will be the same as the last observed value. It is a baseline method often used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Naive forecast\n",
    "naive_forecast = airline_data.shift(1)\n",
    "\n",
    "naive_forecast_mse = mean_squared_error(airline_data[1:], naive_forecast[1:])\n",
    "print(f'Naive forecast MSE: {naive_forecast_mse.values[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**:\n",
    "Implement the Naive method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blank with the correct method to implement the Naive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive forecast\n",
    "temp_naive_forecast = \n",
    "\n",
    "temp_naive_forecast_mse = \n",
    "print(f'Naive forecast MSE: {temp_naive_forecast_mse.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Average Method**: This method forecasts the future value as the mean of all past observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average forecast, the mean of all last observations\n",
    "average_forecast = airline_data.expanding().mean()\n",
    "\n",
    "average_forecast_mse = mean_squared_error(airline_data, average_forecast)\n",
    "\n",
    "print(f'Average forecast MSE: {average_forecast_mse.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**:\n",
    "Implement the Average method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blank with the correct method to implement the Average method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average forecast, the mean of all last observations\n",
    "temp_average_forecast = temp_data.expanding().mean()\n",
    "\n",
    "temp_average_forecast_mse = mean_squared_error(temp_data, temp_average_forecast)\n",
    "\n",
    "print(f'Average forecast MSE: {temp_average_forecast_mse.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Drift Method**: This method assumes a constant trend (drift) in the data and forecasts based on the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Drift forecast\n",
    "drift = np.polyfit(np.arange(len(airline_data)), airline_data['Passengers'], 1)[0]\n",
    "drift_forecast = airline_data.shift(1) + drift\n",
    "\n",
    "drift_forecast_mse = mean_squared_error(airline_data[1:], drift_forecast[1:])\n",
    "print(f'Drift forecast MSE: {drift_forecast_mse.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**:\n",
    "Implement the Drift method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blanks with the correct method to implement the Drift method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift forecast\n",
    "temp_drift = \n",
    "temp_drift_forecast = temp_data.shift(1) + temp_drift\n",
    "\n",
    "temp_drift_forecast_mse = mean_squared_error(temp_data[1:], temp_drift_forecast[1:])\n",
    "print(f'Drift forecast MSE: {temp_drift_forecast_mse.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exponential Smoothing Methods**\n",
    "\n",
    "#### 1. **Simple Exponential Smoothing**: \n",
    "\n",
    "This method assigns exponentially decreasing weights to past observations, with more recent observations carrying higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_data.index.freq = 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index to split the data\n",
    "split_idx = int(len(airline_data) * 0.8)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "airline_train = airline_data.iloc[:split_idx]\n",
    "airline_test = airline_data.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "# Assuming `airline_train` is your training dataset\n",
    "model = SimpleExpSmoothing(airline_train)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast the next steps equivalent to the test set size\n",
    "steps = len(airline_test)  # Assuming you have a separate test set\n",
    "ses_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate MSE\n",
    "ses_forecast_mse = mean_squared_error(airline_test, ses_forecast)\n",
    "print(f'SES forecast MSE: {ses_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**:\n",
    "Implement the Simple Exponential Smoothing method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Define temp_train as the training data and temp_test as the test data.\n",
    "* Replace the blanks with the correct method to implement Simple Exponential Smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index to split the data\n",
    "split_idx = int(len(temp_data) * 0.8)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "temp_train = temp_data.iloc[:split_idx]\n",
    "temp_test = temp_data.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `airline_train` is your training dataset\n",
    "model = \n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast the next steps equivalent to the test set size\n",
    "steps = \n",
    "temp_ses_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "temp_ses_forecast_mse = mean_squared_error(temp_test, temp_ses_forecast)\n",
    "print(f'SES forecast MSE: {temp_ses_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. **Holt's Method**: \n",
    "\n",
    "\n",
    "#####  Holt's Linear Trend Method\n",
    "\n",
    "Holt's Linear Trend method extends Simple Exponential Smoothing to allow forecasting of data with a trend. This method is suitable for time series data where not only the level of the series is important but also the trend. The method involves two equations: one for the level and one for the trend.\n",
    "\n",
    "##### Components of Holt's Method:\n",
    "\n",
    "1. **Level**: The level equation is similar to Simple Exponential Smoothing, adjusting the series for the trend of the previous time step.\n",
    "\n",
    "2. **Trend**: The trend equation estimates the trend in the data, which is the increase or decrease in the level component from one period to the next.\n",
    "\n",
    "##### Equations:\n",
    "\n",
    "The method consists of two equations:\n",
    "\n",
    "1. **Level Equation**:\n",
    "   $$ \\hat{y}_{t+1|t} = \\alpha y_t + (1 - \\alpha)(l_{t-1} + b_{t-1}) $$\n",
    "   Here, $\\hat{y}_{t+1|t}$ is the forecast for the next period, $y_t$ is the actual value at time $t$, $l_{t-1}$ is the estimated level at time $t-1$, and $b_{t-1}$ is the estimated trend at time $t-1$. The parameter $\\alpha$ controls the smoothing of the level.\n",
    "\n",
    "2. **Trend Equation**:\n",
    "   $$ b_t = \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1} $$\n",
    "   Here, $b_t$ is the estimated trend at time $t$, $l_t$ is the estimated level at time $t$, and $\\beta^*$ is the parameter controlling the smoothing of the trend.\n",
    "\n",
    "##### Forecasting:\n",
    "\n",
    "The forecast for $m$ periods ahead is given by:\n",
    "$$ \\hat{y}_{t+m} = l_t + mb_t $$\n",
    "where $l_t$ is the current level, $b_t$ is the current trend, and $m$ is the number of periods ahead for the forecast.\n",
    "\n",
    "##### Implementation in Python:\n",
    "\n",
    "Holt's method can be implemented using the `Holt` class from the `statsmodels.tsa.holtwinters` module in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "# Holt's Method\n",
    "model = Holt(airline_train)\n",
    "fitted_model = model.fit()\n",
    "steps = len(airline_test)\n",
    "holt_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_mse_forecast = mean_squared_error(airline_test, holt_forecast)\n",
    "\n",
    "print(f'Holt forecast MSE: {holt_mse_forecast}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**:\n",
    "Implement Holt's Linear Trend method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Perform the Holt's method on the temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt's Method\n",
    "model = \n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast the next steps equivalent to the test set size\n",
    "steps = \n",
    "temp_holt_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_holt_forecast_mse = mean_squared_error(temp_test, temp_holt_forecast)\n",
    "\n",
    "print(f'Holt forecast MSE: {temp_holt_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. **Holt-Winters Method**: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holt-Winters Method**\n",
    "The Holt-Winters method is a time series forecasting technique that accounts for seasonality and trend. It has three components: level, trend, and seasonality.\n",
    "\n",
    "**Components**\n",
    "* Level (α): Smoothing parameter for the level (0 ≤ α ≤ 1)\n",
    "* Trend (β): Smoothing parameter for the trend (0 ≤ β ≤ 1)\n",
    "* Seasonality (γ): Smoothing parameter for the seasonality (0 ≤ γ ≤ 1)\n",
    "\n",
    "**Formulas**:\n",
    "**Additive Holt-Winters**\n",
    "- Level: $\\ell_t = \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})$\n",
    "- Trend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}$\n",
    "- Seasonality: $s_t = \\gamma(y_t - \\ell_t) + (1 - \\gamma)s_{t-m}$\n",
    "- Forecast: $\\hat{y}_{t+h} = \\ell_t + hb_t + s_{t-m+h}$\n",
    "\n",
    "\n",
    "**Multiplicative Holt-Winters**\n",
    "- Level: $\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})$\n",
    "- Trend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}$\n",
    "- Seasonality: $s_t = \\gamma\\frac{y_t}{\\ell_t} + (1 - \\gamma)s_{t-m}$\n",
    "- Forecast: $\\hat{y}_{t+h} = (\\ell_t + hb_t)s_{t-m+h}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_t$ is the actual value at time $t$\n",
    "- $m$ is the number of periods per season\n",
    "- $h$ is the number of periods ahead to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Holt-Winters Method\n",
    "model = ExponentialSmoothing(airline_train, trend='add', seasonal='add', seasonal_periods=12)\n",
    "fitted_model = model.fit()\n",
    "\n",
    "\n",
    "steps = len(airline_test)\n",
    "holtwinter_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holtwinter_forecast_mse = mean_squared_error(airline_test, holtwinter_forecast)\n",
    "print(f'Holt-Winters forecast MSE: {holtwinter_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12**:\n",
    "Implement the Holt-Winters method for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Perform the Holt-Winters method on the temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \n",
    "fitted_model = model.fit()\n",
    "\n",
    "steps = \n",
    "temp_holtwinter_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_holtwinter_forecast_mse = mean_squared_error(temp_test, temp_holtwinter_forecast)\n",
    "print(f'Holt-Winters forecast MSE: {temp_holtwinter_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoregressive Integrated Moving Average (ARIMA) Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA is a time series forecasting method that combines autoregressive (AR), differencing (I), and moving average (MA) components. It is denoted as ARIMA(p, d, q), where:\n",
    "* p: Order of the autoregressive term\n",
    "* d: Degree of differencing\n",
    "* q: Order of the moving average term\n",
    "\n",
    "**Formulas**:\n",
    "**Autoregressive (AR) Term**\n",
    "$AR(p): y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t$\n",
    "\n",
    "**Differencing (I)**\n",
    "$\\nabla^d y_t = (1 - B)^d y_t$\n",
    "\n",
    "where $B$ is the backshift operator: $B^i y_t = y_{t-i}$\n",
    "\n",
    "**Moving Average (MA) Term**\n",
    "$MA(q): y_t = c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots + \\theta_q \\varepsilon_{t-q}$\n",
    "\n",
    "**Combined ARIMA(p, d, q) Model**\n",
    "$\\phi(B)(1 - B)^d y_t = c + \\theta(B)\\varepsilon_t$\n",
    "\n",
    "where:\n",
    "* $\\phi(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\ldots - \\phi_p B^p$ is the autoregressive operator\n",
    "* $\\theta(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\ldots + \\theta_q B^q$ is the moving average operator\n",
    "* $\\varepsilon_t$ is white noise (error term)\n",
    "* $c$ is a constant\n",
    "\n",
    "The ARIMA model is fitted by estimating the parameters $\\phi_1, \\ldots, \\phi_p, \\theta_1, \\ldots, \\theta_q$ using maximum likelihood estimation or other optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ARIMA model\n",
    "model = ARIMA(airline_train, order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "steps = len(airline_test)\n",
    "arima_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_forecast_mse = mean_squared_error(airline_test, arima_forecast)\n",
    "print(f'ARIMA forecast MSE: {arima_forecast_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 13**:\n",
    "Implement an ARIMA model for forecasting the Daily Minimum Temperatures dataset.\n",
    "\n",
    "**Instructions**:\n",
    "* Replace the blanks with the correct method to implement the ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \n",
    "fitted_model = model.fit()\n",
    "\n",
    "steps = \n",
    "temp_arima_forecast = fitted_model.forecast(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arima_forecast_mse = mean_squared_error(temp_test, temp_arima_forecast)\n",
    "print(f'ARIMA forecast MSE: {temp_arima_forecast_mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
