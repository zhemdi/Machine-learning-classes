{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Review of Time Series Data](#Review-of-Time-Series-Data)\n",
    "3. [Review of Deep Learning Concepts](#Review-of-Deep-Learning-Concepts)\n",
    "4. [Environment Setup](#Environment-Setup)\n",
    "5. [Data Exploration and Preparation](#Data-Exploration-and-Preparation)\n",
    "6. [Constructing a Simple LSTM Model](#Constructing-a-Simple-LSTM-Model)\n",
    "   - [Implementation on Dataset A](#Implementation-on-Dataset-A)\n",
    "   - [Exercise on Dataset B](#Exercise-on-Dataset-B)\n",
    "7. [Model Evaluation Techniques](#Model-Evaluation-Techniques)\n",
    "8. [Exploring Advanced Deep Learning Models](#Exploring-Advanced-Deep-Learning-Models)\n",
    "   - [CNNs](#CNNs)\n",
    "   - [Transformers](#Transformers)\n",
    "9. [Conclusion](#Conclusion)\n",
    "10. [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial focuses on applying deep learning techniques to time series data. It is designed for individuals who have an introductory knowledge of both deep learning and time series analysis. The subsequent sections will delve into practical applications and enhancements of this knowledge by introducing more sophisticated deep learning architectures. \n",
    "\n",
    "Each concept and model discussed will be illustrated with an example. Following the example, exercises are provided to apply similar techniques on a different dataset. Draft cells with hints are included to facilitate the application of the concepts.\n",
    "\n",
    "The aim of this guide is to enable a comprehensive understanding of the application of deep learning models to time series data, emphasizing practical skills and critical analysis of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review of Time Series Data\n",
    "Time series data consists of sequences of values recorded over intervals of time. This type of data is distinguished by its chronological order, which is crucial in analysis as the time component often carries essential structural information that can help in predicting future values. Common examples include financial market data, weather readings, and Internet of Things (IoT) sensor data. Understanding the characteristics of time series data, such as trend, seasonality, and cyclic behavior, is vital for effective modeling and forecasting.\n",
    "\n",
    "## 3. Review of Deep Learning Concepts\n",
    "Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data. For time series, deep learning models can learn to predict future values from past data. Key concepts in deep learning include layers, neurons, activation functions, and backpropagation. Specialized structures like Long Short-Term Memory (LSTM) units and Gated Recurrent Units (GRU) are particularly suited for time series data due to their ability to capture temporal dependencies and forget irrelevant parts of the data over time.\n",
    "\n",
    "## 4. Environment Setup\n",
    "Before diving into the models, it is necessary to set up a Python environment equipped with libraries essential for data handling and modeling. Use the following commands to install the required libraries using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy pandas matplotlib tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the Python environment is correctly set up by importing these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Exploration and Preparation\n",
    "This section will utilize two datasets for deep learning applications. The first dataset, from NOAA, contains hourly weather observations, and the second dataset includes daily stock prices from Google available on Kaggle.\n",
    "\n",
    "### Loading and Visualizing the Weather Data\n",
    "The NOAA dataset provides hourly weather observations that are ideal for demonstrating time series analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('https://www.ncei.noaa.gov/data/global-summary-of-the-day/doc/sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])\n",
    "weather_df.plot(y='TEMP', x='DATE', title='Hourly Temperature Observations')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Â°C)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Loading and Visualizing the Stock Market Data\n",
    "The Google stock price data will be used to apply the concepts learned from the weather data to a financial time series.\n",
    "1. Load the Google stock price data from the provided CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_url = 'https://raw.githubusercontent.com/zhemdi/Machine-learning-classes/master/GOOGL.csv'\n",
    "stock_df = pd.read_csv(stock_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stock_df.plot(y='Close', x='Date', title='Google Stock Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price (USD)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "Both datasets will undergo cleaning, normalization, and transformation into a format suitable for LSTM modeling, including sequence generation and splitting into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the weather data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "weather_scaled = scaler.fit_transform(weather_df[['TEMP']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Data Preparation\n",
    "1. Normalize the stock price data using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the stock data\n",
    "stock_scaled = scaler.fit_transform(stock_df[['Close']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data)-sequence_length):\n",
    "        xs.append(data[i:i+sequence_length])\n",
    "        ys.append(data[i+sequence_length])\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60  # Using last 60 hours/days to predict the next hour/day\n",
    "weather_input, weather_target = create_sequences(weather_scaled, sequence_length)\n",
    "weather_num_observations = weather_input.shape[0]\n",
    "test_size = 0.2\n",
    "weather_train, weather_target_train = weather_input[:int(weather_num_observations*(1-test_size))], weather_target[:int(weather_num_observations*(1-test_size))]\n",
    "weather_test, weather_target_test = weather_input[int(weather_num_observations*(1-test_size)):], weather_target[int(weather_num_observations*(1-test_size)):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: train-test split of the stock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_input, stock_target = create_sequences(stock_scaled, sequence_length)\n",
    "stock_num_observations = stock_input.shape[0]\n",
    "stock_train, stock_target_train = stock_input[:int(stock_num_observations*(1-test_size))], stock_target[:int(stock_num_observations*(1-test_size))]\n",
    "stock_test, stock_target_test = stock_input[int(stock_num_observations*(1-test_size)):], stock_target[int(stock_num_observations*(1-test_size)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constructing a Simple LSTM Model\n",
    "Using the preprocessed data, LSTM models will be constructed to predict future values based on past observations.\n",
    "\n",
    "### Implementation on Dataset A (Weather Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Prepare sequence data for the weather dataset...\n",
    "weather_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "    # LSTM(64, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "weather_model.fit(weather_train, weather_target_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: leverage the LSTM architecture to forecast future stock prices using the preprocessed Google stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence data for the stock dataset...\n",
    "stock_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "    LSTM(64),\n",
    "    Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "stock_model.fit(stock_train, stock_target_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation Techniques\n",
    "After training the LSTM models on the weather and stock datasets, it is critical to evaluate their predictive accuracy to understand how well they are likely to perform in practical scenarios. This section will outline common evaluation techniques for time series forecasting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Making predictions on the test set\n",
    "weather_predictions = weather_model.predict(weather_test)\n",
    "\n",
    "# Calculating RMSE and MAE\n",
    "weather_rmse = mean_squared_error(weather_target_test, weather_predictions, squared=False)\n",
    "weather_mae = mean_absolute_error(weather_target_test, weather_predictions)\n",
    "\n",
    "print(f\"Weather Data Test RMSE: {weather_rmse:.3f}\")\n",
    "print(f\"Weather Data Test MAE: {weather_mae:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(weather_target_test, label='Actual')\n",
    "plt.plot(weather_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Temperature')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Evaluating the Google Stock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "stock_predictions = stock_model.predict(stock_test)\n",
    "\n",
    "# Calculating RMSE and MAE\n",
    "stock_rmse = mean_squared_error(stock_target_test, stock_predictions, squared=False)\n",
    "stock_mae = mean_absolute_error(stock_target_test, stock_predictions)\n",
    "\n",
    "print(f\"Google Stock Data Test RMSE: {stock_rmse:.3f}\")\n",
    "print(f\"Google Stock Data Test MAE: {stock_mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(stock_target_test, label='Actual')\n",
    "plt.plot(stock_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Google Stock Prices')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Deep Learning Models\n",
    "While LSTMs are highly effective for sequence prediction tasks, incorporating Convolutional Neural Networks (CNNs) can provide a different approach that exploits spatial hierarchies in data. CNNs are particularly adept at identifying patterns in data, making them useful for time series forecasting where patterns across time steps dictate future outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs\n",
    "CNNs can process time series data by treating sequences as one-dimensional spatial inputs. This allows the model to capture temporal dependencies using filters and pooling layers. Here's how to implement a simple CNN for the NOAA weather dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Building the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(weather_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cnn_model.fit(weather_train, weather_target_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: CNNs on Google Stock Data\n",
    "Leverage the CNN architecture to forecast future stock prices using the Google stock data. This exercise will help solidify understanding of how CNNs can be applied to different types of time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the CNN model for the stock dataset\n",
    "cnn_stock_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(stock_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_stock_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cnn_stock_model.fit(stock_test, stock_target_test, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "Transformers can be effectively applied to time series data by treating the sequence of data points as a series of inputs that the model can attend to simultaneously. Here's how to implement a simple Transformer model for the NOAA weather dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_positional_encoding(max_seq_len, embed_dim):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j//2) / embed_dim) for j in range(embed_dim)]\n",
    "        if pos != 0 else np.zeros(embed_dim) \n",
    "        for pos in range(max_seq_len)\n",
    "        ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return tf.cast(pos_enc, dtype=tf.float32)\n",
    "\n",
    "# Usage\n",
    "sequence_length = 60  # or the length of your time series data\n",
    "embedding_dim = 128   # or the embedding size of your model\n",
    "\n",
    "positional_encoding = get_positional_encoding(sequence_length, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class TimeSeriesTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, max_len, rate=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(input_dim),\n",
    "        ])\n",
    "        self.pos_enc = get_positional_encoding(max_len, input_dim)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Add batch dimension to positional encoding and repeat it to match the batch size of inputs\n",
    "        pos_enc = tf.expand_dims(self.pos_enc, 0)  # shape becomes [1, seq_length, embed_dim]\n",
    "        pos_enc = tf.tile(pos_enc, [tf.shape(inputs)[0], 1, 1])  # shape becomes [batch_size, seq_length, embed_dim]\n",
    "\n",
    "        # Concatenate inputs with positional encodings\n",
    "        inputs = tf.concat([inputs, pos_enc], axis=-1)  # Ensure inputs is [batch_size, seq_length, 1] before this step\n",
    "\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "input_layer = Input(shape=(sequence_length, 1))\n",
    "x = TimeSeriesTransformer(1, num_heads=8, ff_dim=256, max_len=sequence_length)(input_layer)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(1)(x)\n",
    "transformer_model = Model(inputs=input_layer, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transformer_model.fit(weather_train, weather_target_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Transformers on Google Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "input_layer = Input(shape=(sequence_length, 1))\n",
    "x = TimeSeriesTransformer(1, num_heads=8, ff_dim=256, max_len=sequence_length)(input_layer)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(1)(x)\n",
    "transformer_stock_model = Model(inputs=input_layer, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_stock_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "transformer_stock_model.fit(stock_train, stock_target_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of CNNs and Transformers\n",
    "Model evaluation is key to determining the effectiveness of CNNs and Transformers in forecasting time series data. This subsection will detail the evaluation process for both models on the NOAA weather data and Google stock data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the CNN on NOAA Weather Data\n",
    "After training the CNN model on the weather data, we assess its performance using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), as well as visual comparisons of actual vs. predicted values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "weather_cnn_predictions = cnn_model.predict(weather_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "weather_cnn_rmse = mean_squared_error(weather_target_test, weather_cnn_predictions, squared=False)\n",
    "weather_cnn_mae = mean_absolute_error(weather_target_test, weather_cnn_predictions)\n",
    "\n",
    "print(f\"Weather Data CNN RMSE: {weather_cnn_rmse:.3f}\")\n",
    "print(f\"Weather Data CNN MAE: {weather_cnn_mae:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(weather_target_test, label='Actual')\n",
    "plt.plot(weather_cnn_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Temperature - CNN Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Evaluating the CNN on Google Stock Data\n",
    "Similarly, evaluate the CNN model trained on the Google stock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "stock_cnn_predictions = cnn_stock_model.predict(stock_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "stock_cnn_rmse = mean_squared_error(stock_target_test, stock_cnn_predictions, squared=False)\n",
    "stock_cnn_mae = mean_absolute_error(stock_target_test, stock_cnn_predictions)\n",
    "\n",
    "print(f\"Stock Data CNN RMSE: {stock_cnn_rmse:.3f}\")\n",
    "print(f\"Stock Data CNN MAE: {stock_cnn_mae:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(stock_target_test, label='Actual')\n",
    "plt.plot(stock_cnn_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Stock Prices - CNN Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Transformer on NOAA Weather Data\n",
    "Evaluate the performance of the Transformer model on the weather dataset using similar metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "weather_transformer_predictions = transformer_model.predict(weather_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "weather_transformer_rmse = mean_squared_error(weather_target_test, weather_transformer_predictions, squared=False)\n",
    "weather_transformer_mae = mean_absolute_error(weather_target_test, weather_transformer_predictions)\n",
    "\n",
    "print(f\"Weather Data Transformer RMSE: {weather_transformer_rmse:.3f}\")\n",
    "print(f\"Weather Data Transformer MAE: {weather_transformer_mae:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(weather_target_test, label='Actual')\n",
    "plt.plot(weather_transformer_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Temperature - Transformer Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Temperature')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Evaluating the Transformer on Google Stock Data\n",
    "Finally, assess how the Transformer model performs on the Google stock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "stock_transformer_predictions = transformer_stock_model.predict(stock_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "stock_transformer_rmse = mean_squared_error(stock_target_test, stock_transformer_predictions, squared=False)\n",
    "stock_transformer_mae = mean_absolute_error(stock_target_test, stock_transformer_predictions)\n",
    "\n",
    "print(f\"Stock Data Transformer RMSE: {stock_transformer_rmse:.3f}\")\n",
    "print(f\"Stock Data Transformer MAE: {stock_transformer_mae:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(stock_target_test, label='Actual')\n",
    "plt.plot(stock_transformer_predictions, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Stock Prices - Transformer Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has systematically explored the application of deep learning methods to time series data. It began with foundational concepts in data preparation and visualization, followed by the development of various deep learning models suited for time series analysis.\n",
    "\n",
    "### Summary of Key Points:\n",
    "- **Data Preparation**: Emphasis was placed on the significance of data normalization and sequence creation, essential for adapting time series data for neural network inputs.\n",
    "- **Model Development**: Techniques such as Long Short-Term Memory (LSTM) networks were detailed, highlighting their utility in capturing temporal dependencies. Further, advanced neural architectures, including Convolutional Neural Networks (CNNs) and Transformers, were adapted for time series data, exploiting their capabilities in recognizing spatial patterns and managing sequence relationships through self-attention mechanisms, respectively.\n",
    "- **Evaluation Methods**: The tutorial introduced methods for assessing model performance using quantitative metrics like Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), in addition to visual analysis techniques. These methods are critical for validating the effectiveness of predictive models in time series analysis.\n",
    "\n",
    "### Recommendations for Further Study:\n",
    "- **Hyperparameter Tuning**: There is substantial scope for improving model outcomes through the fine-tuning of hyperparameters such as layer configurations, neuron counts, and learning rates.\n",
    "- **Integration of Model Architectures**: Exploring hybrid models that combine the strengths of different neural network types could potentially enhance predictive accuracy by capturing a broader range of data patterns.\n",
    "- **Diversification of Data Sources**: Utilizing datasets from varied domains can increase the generalizability and robustness of predictive models. This diversification supports the development of models capable of handling complex, real-world datasets characterized by noise, non-stationarity, and heterogeneity.\n",
    "- **Deployment in Operational Environments**: Implementing these models within real-time data processing frameworks can provide insights into their performance under dynamic conditions, highlighting potential areas for model refinement.\n",
    "\n",
    "### Advancing Your Expertise in Deep Learning for Time Series\n",
    "Deep learning for time series data is a dynamic area of research with significant implications across numerous sectors, including finance, environmental science, and healthcare. Ongoing education in the latest research methodologies and active participation in scholarly discussions are recommended to maintain a cutting-edge understanding of the field.\n",
    "\n",
    "This tutorial aimed to provide a robust introduction to the methodologies applicable in this sphere, setting a foundation for further investigation and innovation in deep learning for time series analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
