{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digits Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will work on the famous MNIST dataset, which contains images of handwritten digits from 0 to 9. The goal is to build a machine learning model that can recognize and classify the digits accurately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "1. **Introduction**\n",
    "    * Brief description of the MNIST dataset\n",
    "    * Objective of the tutorial\n",
    "\n",
    "2. **Loading and Exploring the Data**\n",
    "    * Loading the MNIST dataset\n",
    "    * Visualizing the data\n",
    "    * Analyzing the distribution of digits in the dataset\n",
    "3. **Data Preprocessing**\n",
    "    * Normalizing the pixel values\n",
    "    * One-hot encoding the labels\n",
    "    * Splitting the dataset into training and testing sets\n",
    "4. **Building the Model**\n",
    "    * Selecting a suitable machine learning algorithm\n",
    "    * Implementing the model using scikit-learn\n",
    "5. **Training the Model**\n",
    "    * Training the model on the training dataset\n",
    "    * Evaluating the performance using cross-validation\n",
    "6. **Model Evaluation**\n",
    "    * Assessing the performance of the model on the test dataset\n",
    "    * Analyzing the confusion matrix\n",
    "    * Visualizing the misclassified digits\n",
    "7. **Improving the Model**\n",
    "    * Feature engineering techniques\n",
    "    * Hyperparameter tuning\n",
    "    * Implementing a more complex model (e.g., Convolutional Neural Network)\n",
    "8. **Deploying the Model**\n",
    "    * Saving the trained model\n",
    "    * Loading the model for inference\n",
    "    * Creating a simple application for digit recognition\n",
    "9. **Conclusion**\n",
    "    * Recap of the tutorial\n",
    "    * Potential applications of digit recognition\n",
    "    * Further resources and recommendations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this tutorial, we will be working with the famous MNIST (Modified National Institute of Standards and Technology) dataset. The MNIST dataset is a large database of handwritten digits, containing 60,000 training samples and 10,000 testing samples. Each sample is a grayscale image of size 28x28 pixels, representing a digit from 0 to 9.\n",
    "\n",
    "![MNIST Sample](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "*Sample images from the MNIST dataset.*\n",
    "\n",
    "The objective of this tutorial is to guide you through the process of building a machine learning model that can recognize and classify these handwritten digits accurately. We will explore various techniques for data preprocessing, model building, training, and evaluation. By the end of this tutorial, you will have a solid understanding of how to work with image data and build a classifier for the MNIST dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Exploring the Data\n",
    "### 2.1. Loading the MNIST dataset\n",
    "The MNIST dataset is widely used in machine learning and can be easily loaded using popular libraries like *TensorFlow* or *scikit-learn*. In this tutorial, we will use *scikit-learn* to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Try loading the MNIST dataset using TensorFlow. Look up the relevant function in the *TensorFlow* documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Import TensorFlow, then use the `load_data()` function from the `tensorflow.keras.datasets.mnist` module to load the MNIST dataset.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Code sketch:\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Visualizing the data\n",
    "Let's visualize some of the handwritten digits in the dataset to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot a single digit\n",
    "plot_digit(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Visualize 5 random digits from the dataset in a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hint: Generate 5 random indices using `np.random.randint()`. Create a 1x5 subplots using `plt.subplots()`, and use a for loop to call the `plot_digit()` function for each index and corresponding subplot.\n",
    "# Code sketch:\n",
    "# rand_indices = np.random.randint(0, len(X), 5)\n",
    "# fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "# for i, ax in zip(rand_indices, axes):\n",
    "#     plot_digit(X[i], ax)\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Analyzing the distribution of digits in the dataset\n",
    "It's important to analyze the distribution of the target variable (in this case, the digits) to ensure that the dataset is balanced and representative of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(y)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Digits in the MNIST Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display a bar plot showing the distribution of the digits in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Calculate the percentage of each digit in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hint: Convert the target variable `y` to a pandas Series, use the `value_counts()` method with the `normalize` parameter set to True, and multiply the result by 100.\n",
    "# Code sketch:\n",
    "# y_series = pd.Series(y)\n",
    "# percentages = y_series.value_counts(normalize=True) * 100\n",
    "# print(percentages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "### 3.1. Normalizing the pixel values\n",
    "To improve the performance of our machine learning models, we can normalize the pixel values of our images. Normalization rescales the pixel values to a range of [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values\n",
    "X_normalized = X / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "Try normalizing the pixel values using different methods, such as min-max scaling or standard scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: For min-max scaling, use (X - X.min()) / (X.max() - X.min()). For standard scaling, use (X - X.mean()) / X.std().\n",
    "# Code sketch:\n",
    "# Min-max scaling:\n",
    "# X_min_max_scaled = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Standard scaling:\n",
    "# X_standard_scaled = (X - X.mean()) / X.std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. One-hot encoding the labels\n",
    "We need to convert the labels to one-hot encoded vectors to use them in our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "Try using TensorFlow's '**to_categorical()**' function to convert the labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Import the necessary function from `tensorflow.keras.utils`, then call the function with the `y` variable as its argument.\n",
    "# Code sketch:\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_one_hot_tf = to_categorical(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Splitting the dataset into training and testing sets\n",
    "To evaluate our model's performance, we will split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "Split the dataset into training, validation, and testing sets, with a 60-20-20 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Call the `train_test_split()` function twice, first to split the data into a 60-40 distribution, and then split the 40% portion into equal parts for validation and testing.\n",
    "# Code sketch:\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Model\n",
    "### 4.1.Selecting a suitable machine learning algorithm\n",
    "For the MNIST handwritten digits classification, we will use a simple yet effective machine learning algorithm: the Support Vector Machine (SVM). SVMs are suitable for this task because they can handle multi-class classification problems and work well with high-dimensional data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Implementing the model using scikit-learn\n",
    "To implement the SVM model, we will use the '**SVC**' class from the '**sklearn.svm**' module. We will train the model using the '**fit**' method, providing the training data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "Try different kernel functions, such as linear, polynomial, and sigmoid, and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Create new instances of the `SVC` class, providing the `kernel` parameter with the desired kernel function, and train the model.\n",
    "# Code sketch:\n",
    "# Linear kernel:\n",
    "# svm_linear = SVC(kernel=\"linear\", random_state=42)\n",
    "# svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial kernel:\n",
    "# svm_poly = SVC(kernel=\"poly\", random_state=42)\n",
    "# svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Sigmoid kernel:\n",
    "# svm_sigmoid = SVC(kernel=\"sigmoid\", random_state=42)\n",
    "# svm_sigmoid.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "Train a Random Forest Classifier and compare its performance with the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Import the `RandomForestClassifier` class from `sklearn.ensemble`, create an instance of the classifier, and train the model.\n",
    "# Code sketch:\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "# rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "### 5.1. Training the model on the training dataset\n",
    "Now that we have our model defined, it's time to train it on the training dataset. We will use the '**fit**' method of the '**SVC**' class to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaluating the performance using cross-validation\n",
    "To evaluate the performance of our model, we can use cross-validation. Cross-validation involves splitting the dataset into several folds and training and evaluating the model on each fold. We will use the cross_val_score function from the sklearn.model_selection module to perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation on the SVM model\n",
    "cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the mean and standard deviation of the cross-validation scores\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation of cross-validation scores: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "Evaluate the performance of the models with different kernel functions and the Random Forest Classifier using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the `cross_val_score` function to compute the cross-validation scores for each model.\n",
    "# Code sketch:\n",
    "# Linear kernel:\n",
    "# cv_scores_linear = cross_val_score(svm_linear, X_train, y_train, cv=5)\n",
    "\n",
    "# Polynomial kernel:\n",
    "# cv_scores_poly = cross_val_score(svm_poly, X_train, y_train, cv=5)\n",
    "\n",
    "# Sigmoid kernel:\n",
    "# cv_scores_sigmoid = cross_val_score(svm_sigmoid, X_train, y_train, cv=5)\n",
    "\n",
    "# Random Forest Classifier:\n",
    "# cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "Experiment with different values of the '**C**' parameter for the SVM model and observe the effect on the cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Create new instances of the `SVC` class, providing different values for the `C` parameter, and compute the cross-validation scores.\n",
    "# Code sketch:\n",
    "# svm_c1 = SVC(C=1, random_state=42)\n",
    "# cv_scores_c1 = cross_val_score(svm_c1, X_train, y_train, cv=5)\n",
    "\n",
    "# svm_c10 = SVC(C=10, random_state=42)\n",
    "# cv_scores_c10 = cross_val_score(svm_c10, X_train, y_train, cv=5)\n",
    "\n",
    "# svm_c100 = SVC(C=100, random_state=42)\n",
    "# cv_scores_c100 = cross_val_score(svm_c100, X_train, y_train, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "### 6.1. Assessing the performance of the model on the test dataset\n",
    "After training the model and evaluating it using cross-validation, it's time to assess its performance on the test dataset. We will use the '**predict**' method of the '**SVC**' class to make predictions on the test dataset and then compute the accuracy of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Analyzing the confusion matrix\n",
    "A confusion matrix can help us understand the performance of the classifier in more detail. We will use the '**confusion_matrix**' function from the '**sklearn.metrics**' module to compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Visualizing the misclassified digits\n",
    "To better understand the misclassifications, we can visualize some of the misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find the misclassified digits\n",
    "misclassified_indices = np.where(y_test != y_pred)\n",
    "\n",
    "# Plot the first 9 misclassified digits\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    index = misclassified_indices[0][i]\n",
    "    ax.imshow(X_test[index].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"True: {y_test[index]}, Predicted: {y_pred[index]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    "Compute the precision, recall, and F1-score for each class using the '**classification_report**' function from the '**sklearn.metrics**' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the `classification_report` function to compute the precision, recall, and F1-score for each class.\n",
    "# Code sketch:\n",
    "# from sklearn.metrics import classification_report\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "Visualize the misclassified digits for a different classifier, such as the Random Forest Classifier, and compare the types of errors made by the two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Train a Random Forest Classifier, make predictions on the test dataset, find the misclassified digits, and plot them.\n",
    "# Code sketch:\n",
    "# Train a Random Forest Classifier\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Find the misclassified digits\n",
    "# misclassified_indices_rf = np.where(y_test != y_pred_rf)\n",
    "\n",
    "# Plot the first 9 misclassified digits for the Random Forest Classifier\n",
    "# (use the code from the \"Visualizing the misclassified digits\" section, replacing the misclassified_indices variable with misclassified_indices_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improving the Model\n",
    "### 7.1. Feature engineering techniques\n",
    "Feature engineering is the process of creating new features or transforming existing features to improve the performance of a machine learning model. Some common techniques for feature engineering with image data include:\n",
    "* Image augmentation (rotations, flips, shifts, etc.)\n",
    "* Dimensionality reduction (e.g., PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of image augmentation using rotation\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Rotate an image by 15 degrees clockwise\n",
    "rotated_image = rotate(X_train[0].reshape(28, 28), -15, reshape=False)\n",
    "\n",
    "# Plot the original and rotated image\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(rotated_image, cmap='gray')\n",
    "plt.title(\"Rotated Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Hyperparameter tuning\n",
    "Hyperparameter tuning involves finding the optimal set of hyperparameters for a machine learning model. For the case of our SVM classifier, we can use a technique like Grid Search or Randomized Search to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training dataset\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Implementing a more complex model (e.g., Convolutional Neural Network)\n",
    "Convolutional Neural Networks (CNNs) are a type of deep learning model that have shown great success in image classification tasks. To implement a CNN, we can use a deep learning library like TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple CNN using TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Create a CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train.reshape(-1, 28, 28, 1), y_train_one_hot, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "Try applying different feature engineering techniques to the dataset, such as image augmentation or dimensionality reduction, and evaluate the performance of the model after applying these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Apply the chosen feature engineering technique to the training dataset and retrain the model.\n",
    "# Code sketch:\n",
    "# Apply feature engineering technique (e.g., image augmentation, PCA, etc.)\n",
    "# Retrain the model using the transformed dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14\n",
    "Perform hyperparameter tuning for a different classifier, such as Random Forest or K-Nearest Neighbors, and compare the performance with the tuned SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Define the hyperparameters and their possible values for the chosen classifier and perform a GridSearchCV or RandomizedSearchCV.\n",
    "# Code sketch:\n",
    "# Import the classifier (e.g., RandomForestClassifier, KNeighborsClassifier, etc.)\n",
    "# Define the hyperparameters and their possible values\n",
    "# Create a GridSearchCV or RandomizedSearchCV object\n",
    "# Perform the search on the training dataset\n",
    "# Print the best hyperparameters and compare the performance with the tuned SVM model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15\n",
    "Implement a more complex CNN architecture and compare its performance with the simpler CNN model provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Add more layers or increase the number of filters in the Conv2D layers.\n",
    "# Code sketch:\n",
    "# Create a more complex CNN model (e.g., add more layers, increase the number of filters, etc.)\n",
    "# Compile the model\n",
    "# Train the model\n",
    "# Evaluate the model's performance and compare it with the simpler CNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploying the Model\n",
    "### 8.1. Saving the trained model\n",
    "Once we have trained and fine-tuned our model, we need to save it so that it can be used for inference later on. In this section, we'll learn how to save the trained model and load it for use in an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'mnist_svm_model.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16\n",
    "Save the improved CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Save the improved CNN model using the 'save' method.\n",
    "# Code sketch:\n",
    "# model.save('mnist_cnn_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Loading the model for inference\n",
    "After saving the model, we need to load it to use for predictions. Here's how we can load the saved model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('mnist_svm_model.pkl')\n",
    "\n",
    "# Use the loaded model for inference\n",
    "sample_digit = X_test[0]\n",
    "prediction = loaded_model.predict(sample_digit.reshape(1, -1))\n",
    "print(\"Predicted digit:\", prediction[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17\n",
    "Load the saved CNN model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Load the saved CNN model using the 'load_model' method from tensorflow.keras.models\n",
    "# Code sketch:\n",
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_cnn_model = load_model('mnist_cnn_model.h5')\n",
    "# Use the loaded CNN model for inference on a sample digit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Creating a simple application for digit recognition\n",
    "Now that we have saved and loaded our model, we can create a simple application that utilizes the model for digit recognition. This can be a web application, a desktop application, or even a mobile application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: For this exercise, we will provide a high-level overview and some code snippets for creating a simple Flask web application.\n",
    "# You'll need to adapt the code and integrate the necessary components for digit recognition.\n",
    "# Note that this exercise is more open-ended and may require you to explore additional documentation.\n",
    "\n",
    "# Code sketch:\n",
    "\n",
    "# Step 1: Install Flask (if not already installed)\n",
    "# !pip install Flask\n",
    "\n",
    "# Step 2: Create a new Flask application file (e.g., app.py) with the following content:\n",
    "\n",
    "# from flask import Flask, render_template, request\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# def preprocess_input(image_data):\n",
    "#     # Preprocess the input image_data here (e.g., normalize the pixel values, reshape the input, etc.)\n",
    "#     return preprocessed_image\n",
    "\n",
    "# @app.route('/', methods=['GET', 'POST'])\n",
    "# def index():\n",
    "#     if request.method == 'POST':\n",
    "#         # Get the image data from the POST request\n",
    "#         image_data = request.form['image_data']\n",
    "        \n",
    "#         # Preprocess the input image data\n",
    "#         preprocessed_image = preprocess_input(image_data)\n",
    "        \n",
    "#         # Load the saved CNN model\n",
    "#         loaded_cnn_model = load_model('mnist_cnn_model.h5')\n",
    "        \n",
    "#         # Use the loaded CNN model for inference on the preprocessed image\n",
    "#         prediction = loaded_cnn_model.predict(preprocessed_image)\n",
    "        \n",
    "#         # Get the predicted digit and return it as a response\n",
    "#         predicted_digit = np.argmax(prediction)\n",
    "#         return str(predicted_digit)\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n",
    "\n",
    "# Step 3: Create an 'index.html' file in a 'templates' folder with a simple user interface that captures the user's input (e.g., drawn digit).\n",
    "\n",
    "# Step 4: Implement the 'preprocess_input' function in 'app.py' to handle the preprocessing of the input image data.\n",
    "\n",
    "# Step 5: Run the Flask application and test the digit recognition functionality.\n",
    "\n",
    "# Note: This code sketch provides a high-level overview of creating a Flask web application for digit recognition. You'll need to adapt the code, create the necessary files, and integrate the components for digit recognition. This exercise requires additional exploration and learning beyond the provided hints.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "### 9.1. Recap of the tutorial\n",
    "In this tutorial, we went through the process of creating a handwritten digit recognition model using the MNIST dataset. We started by loading and exploring the data, followed by preprocessing it for optimal performance. We then built, trained, and evaluated a machine learning model, and even improved it using feature engineering, hyperparameter tuning, and implementing a more complex model such as a Convolutional Neural Network (CNN). Finally, we demonstrated how to deploy the model using a simple Flask web application.\n",
    "\n",
    "### 9.2. Potential applications of digit recognition\n",
    "Handwritten digit recognition has a variety of real-world applications, including:\n",
    "1. Postal mail sorting: recognizing handwritten zip codes on envelopes to facilitate mail sorting.\n",
    "2. Bank check processing: identifying the amount written on checks in order to process transactions.\n",
    "3. Form digitization: converting handwritten numbers on various forms (e.g., surveys, exams) into digital data for further processing and analysis.\n",
    "4. Assistive technology: helping people with disabilities to communicate more effectively by recognizing handwritten numbers.\n",
    "### 9.3. Further resources and recommendations\n",
    "To deepen your understanding and explore other aspects of digit recognition and machine learning, we recommend the following resources:\n",
    "1. [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning) by Andrew Ng: A comprehensive course on deep learning, covering various aspects including Convolutional Neural Networks.\n",
    "2. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas: A useful guide for learning more about data science and machine learning using Python.\n",
    "3. [TensorFlow Tutorials](https://www.tensorflow.org/tutorials): Official TensorFlow tutorials for various machine learning tasks, including digit recognition.\n",
    "By continuing to practice and learn from these resources, you'll be well-equipped to tackle more complex machine learning problems and create even more powerful models. Happy learning!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
