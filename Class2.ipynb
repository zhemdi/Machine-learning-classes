{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for ML Algorithms and Performing Matrix Manipulations Using Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook tutorial, we will cover how to prepare data for machine learning algorithms and perform matrix manipulations using the Pandas library. This tutorial is intended for high school graduates and assumes basic knowledge of **Python**, **Pandas**, **Matplotlib**, and **Seaborn**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Loading the Dataset\n",
    "3. Exploratory Data Analysis\n",
    "    * Descriptive Statistics\n",
    "    * Visualizations\n",
    "4. Data Preprocessing\n",
    "    * Handling Missing Data\n",
    "    * Handling Categorical Data\n",
    "    * Feature Scaling\n",
    "5. Matrix Manipulations\n",
    "    * Selecting and Filtering Data\n",
    "    * Matrix Operations\n",
    "    * Aggregating Data\n",
    "6. Feature Engineering\n",
    "    * Feature Creation\n",
    "    * Feature Selection\n",
    "7. Splitting the Dataset\n",
    "    * Train-Test Split\n",
    "8. Model Selection and Evaluation\n",
    "    * Model Selection\n",
    "    * Model Evaluation\n",
    "9. Model Tuning and Optimization\n",
    "    * Grid Search\n",
    "    * Retrain and Evaluate the Optimized Model\n",
    "10. Conclusion\n",
    "\n",
    "Throughout the tutorial, exercises will be provided at the end of each section for practice and better understanding of the concepts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this tutorial, we will work with the [Wine Quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Repository, which contains information about various wine samples and their quality scores. Our goal is to prepare this dataset for machine learning algorithms and perform matrix manipulations using Pandas.\n",
    "\n",
    "**Objective:**\n",
    "* Learn how to prepare data for machine learning algorithms\n",
    "* Perform matrix manipulations using Pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "First, we need to load the Wine Quality dataset using Pandas. The dataset is available as a CSV file, and we will use the '**pd.read_csv()**' function to read the file and store it in a DataFrame called '**wine_df**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_df = pd.read_csv(url, sep=\";\")\n",
    "wine_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df_copy = wine_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Load the Wine Quality dataset for white wines and display the first five rows. The dataset can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the one for the white wine dataset and read it into a new DataFrame\n",
    "# Hint: The correct URL for the white wine dataset is \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "white_wine_url = ______\n",
    "\n",
    "# Hint: Use the pd.read_csv() function to read the CSV file and set the 'sep' parameter to \";\"\n",
    "white_wine_df = ______\n",
    "\n",
    "# Hint: Use the .head() method to display the first 5 rows of the DataFrame\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wine_df_copy = white_wine_df.copy() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "In this section, we will perform exploratory data analysis to understand the dataset better. We will compute descriptive statistics and visualize the data.\n",
    "\n",
    "### 3.1 Descriptive Statistics\n",
    "We will start by computing the summary statistics for the dataset using the '**describe()**' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Compute and display summary statistics for the white wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe() function on the white_wine_df DataFrame\n",
    "______"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizations\n",
    "We will now create some visualizations to explore the distribution of the features and their relationships. We will use both Matplotlib and Seaborn for creating these visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram of wine quality scores\n",
    "sns.histplot(wine_df['quality'], kde=False, bins=6)\n",
    "plt.title('Wine Quality Distribution')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of alcohol content grouped by wine quality\n",
    "sns.boxplot(x='quality', y='alcohol', data=wine_df)\n",
    "plt.title('Alcohol Content by Wine Quality')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Alcohol')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot of alcohol content vs. wine density\n",
    "sns.scatterplot(x='alcohol', y='density', data=wine_df, hue='quality')\n",
    "plt.title('Alcohol Content vs. Wine Density')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Quality')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Create a scatterplot of alcohol content vs. volatile acidity for the white wine dataset. Use wine quality as the hue for the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scatterplot function with x='alcohol', y='volatile acidity', and hue='quality' on the white_wine_df DataFrame\n",
    "# Hint: Use the sns.scatterplot() function and set x, y, data, and hue parameters accordingly\n",
    "sns._____(x=_____, y=_____, data=_____, hue=_____)\n",
    "\n",
    "# Hint: Use the plt.title() function to set the title of the plot\n",
    "plt._____('Alcohol Content vs. Volatile Acidity')\n",
    "\n",
    "# Hint: Use the plt.xlabel() function to set the x-axis label\n",
    "plt._____('Alcohol')\n",
    "\n",
    "# Hint: Use the plt.ylabel() function to set the y-axis label\n",
    "plt._____('Volatile Acidity')\n",
    "\n",
    "# Hint: Use the plt.legend() function to customize the legend and set the title of the legend\n",
    "plt.legend(title='Quality')\n",
    "\n",
    "# Hint: Use the plt.show() function to display the plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Create a heatmap showing the correlation between the features in the red wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for the wine_df DataFrame\n",
    "# Hint: Use the .corr() method on the wine_df DataFrame\n",
    "corr_matrix = ______\n",
    "\n",
    "# Create a heatmap using the correlation matrix\n",
    "# Hint: Use the sns.heatmap() function with the correlation matrix and set annot and cmap parameters\n",
    "sns._____(corr_matrix, annot=_____, cmap=_____)\n",
    "\n",
    "# Hint: Use the plt.title() function to set the title of the heatmap\n",
    "plt._____('Correlation Heatmap')\n",
    "\n",
    "# Hint: Use the plt.show() function to display the heatmap\n",
    "plt._____()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the tutorial, exercises will be provided at the end of each section for practice and better understanding of the concepts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "In this section, we will perform data preprocessing to clean and prepare the dataset for machine learning algorithms.\n",
    "\n",
    "### 4.1 Handling Missing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check for missing data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there are no missing values in the dataset. However, if there were any missing values, you could handle them using methods like dropping the rows with missing data or imputing the missing values with the mean, median, or mode of the respective feature.\n",
    "\n",
    "**Exercise 5:** Check the white wine dataset for missing data and handle it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in the white wine dataset\n",
    "# Hint: Use the .isnull() method followed by the .sum() method on the white_wine_df DataFrame\n",
    "______.______().______()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handling Categorical Data\n",
    "Our dataset consists of only numerical features, so there is no need to handle categorical data in this case. However, if your dataset contains categorical features, you could use techniques like one-hot encoding or label encoding to convert them into numerical data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Scaling\n",
    "Feature scaling is an important step in preprocessing data for machine learning algorithms. Many algorithms perform better when the features have the same scale. We will use the StandardScaler from the sklearn library to scale our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_wine_df = pd.DataFrame(scaler.fit_transform(wine_df), columns=wine_df.columns)\n",
    "scaled_wine_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Apply feature scaling to the white wine dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features of the white wine dataset using the StandardScaler\n",
    "# Hint: Use the scaler.fit_transform() function on the white_wine_df DataFrame and pass the result to pd.DataFrame()\n",
    "scaled_white_wine_df = pd.DataFrame(______.______(______), columns=______.columns)\n",
    "\n",
    "# Hint: Use the .head() method to display the first 5 rows of the scaled DataFrame\n",
    "scaled_white_wine_df.______()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Manipulations\n",
    "In this section, we will perform matrix manipulations using Pandas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Selecting and Filtering Data\n",
    "\n",
    "We can select specific rows or columns and filter the data based on certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the first 10 rows of the 'quality' column\n",
    "wine_df['quality'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering wines with quality greater than 6\n",
    "high_quality_wines = wine_df[wine_df['quality'] > 6]\n",
    "high_quality_wines.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Select the first 15 rows of the 'pH' column from the white wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 15 rows of the 'pH' column from the white_wine_df DataFrame\n",
    "# Hint: Use the column name 'pH' and the .head() method with 15 as an argument on the white_wine_df DataFrame\n",
    "white_wine_df[_____].______(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Filter the white wine dataset to include only wines with a residual sugar value greater than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the white_wine_df DataFrame to include only wines with a residual sugar value greater than 20\n",
    "# Hint: Use boolean indexing with the condition (white_wine_df['residual sugar'] > 20)\n",
    "sweet_white_wines = white_wine_df[______[___________] > 20]\n",
    "\n",
    "# Hint: Use the .head() method to display the first 5 rows of the filtered DataFrame\n",
    "sweet_white_wines.______()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Matrix Operations\n",
    "\n",
    "We can perform various matrix operations using Pandas, such as adding, subtracting, or multiplying the values in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 to the alcohol content of each wine\n",
    "wine_df['alcohol'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying the alcohol content and pH to create a new feature\n",
    "wine_df['alcohol'] * wine_df['pH']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Create a new feature in the white wine dataset by dividing the 'total sulfur dioxide' by the 'free sulfur dioxide'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the 'total sulfur dioxide' by the 'free sulfur dioxide' in the white_wine_df DataFrame\n",
    "# Hint: Use the column names 'total sulfur dioxide' and 'free sulfur dioxide' for the division operation\n",
    "white_wine_df[_________________] / white_wine_df[__________________]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Subtract the minimum value of the 'density' column from each density value in the red wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the minimum value of the 'density' column from each density value in the wine_df DataFrame\n",
    "# Hint: Use the column name 'density' and the .min() method on the wine_df DataFrame for the subtraction operation\n",
    "wine_df[_______] - wine_df[_______].____()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "In this section, we will create new features that can help improve the performance of our machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Creating New Features\n",
    "\n",
    "We will create a new feature called 'sweetness' by binning the 'residual sugar' values into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the residual sugar values into categories\n",
    "wine_df_with_sweetness = wine_df.copy()\n",
    "wine_df_with_sweetness['sweetness'] = pd.cut(wine_df['residual sugar'], bins=[0, 4, 12, 45], labels=['dry', 'medium', 'sweet'])\n",
    "wine_df_with_sweetness[['residual sugar', 'sweetness']].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11:** Create a new feature in the white wine dataset called 'acidity_level' by binning the 'pH' values into categories such as 'low', 'medium', and 'high'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the pH values into categories\n",
    "# Hint: Create a copy of the white_wine_df DataFrame\n",
    "white_wine_df_with_acidity = white_wine_df.____()\n",
    "\n",
    "# Hint: Use the pd.cut() function with the 'pH' column, bins, and labels as arguments\n",
    "white_wine_df_with_acidity['acidity_level'] = pd.cut(white_wine_df_with_acidity[____], bins=[2.7, 3.0, 3.3, 4.0], labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Hint: Use the .head() method to display the first 5 rows of the 'pH' and 'acidity_level' columns\n",
    "white_wine_df_with_acidity[[____, ____]].____()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Feature Selection\n",
    "\n",
    "Sometimes, it is helpful to remove irrelevant or redundant features from our dataset to improve the performance of our machine learning algorithms. We can use various techniques like correlation analysis, feature importance, or Recursive Feature Elimination (RFE) to select the most important features.\n",
    "\n",
    "**Exercise 12:** Remove the least important feature from the red wine dataset using correlation analysis. You can use the correlation matrix created earlier in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the least correlated feature with the 'quality' column\n",
    "# Hint: Use the .abs() method followed by the .idxmin() method on the 'quality' column of the corr_matrix DataFrame\n",
    "least_correlated_feature = corr_matrix[______].____().____()\n",
    "\n",
    "# Drop the least correlated feature from the wine_df DataFrame\n",
    "# Hint: Use the .drop() method with the columns parameter and the inplace=True argument on the wine_df DataFrame\n",
    "wine_df.drop(columns=[_______________], inplace=_____)\n",
    "\n",
    "# Hint: Use the .head() method to display the first 5 rows of the modified DataFrame\n",
    "wine_df.____()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Splitting the Dataset\n",
    "\n",
    "In this section, we will split the dataset into training and testing sets to evaluate the performance of our machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Train-Test Split\n",
    "\n",
    "We will use the '**train_test_split()**' function from the '**sklearn.model_selection**' module to split our dataset into training and testing sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = wine_df.drop(columns=['quality'])\n",
    "y = wine_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 13:** Split the white wine dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (X) and target variable (y) for the white wine dataset\n",
    "# Hint: Use the .drop() method to exclude the 'quality' column for the features\n",
    "X_white = white_wine_df.____(columns=[_____])\n",
    "# Hint: Use the column name 'quality' to define the target variable\n",
    "y_white = white_wine_df[______]\n",
    "\n",
    "# Split the white wine dataset into training and testing sets\n",
    "# Hint: Use the train_test_split function with the test_size and random_state parameters\n",
    "X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(_____, _____, test_size=_____, random_state=_____)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection and Evaluation\n",
    "\n",
    "In this section, we will select a machine learning model, train it on our training data, and evaluate its performance on the testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Model Selection\n",
    "\n",
    "We will use the Random Forest Classifier from the '**sklearn.ensemble**' module as our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Model Evaluation\n",
    "\n",
    "We will use accuracy as our evaluation metric. We will compute the accuracy of our model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14:** Train a Random Forest Classifier on the white wine dataset and evaluate its performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Random Forest Classifier on the white wine dataset\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_white = RandomForestClassifier(random_state=____)\n",
    "# Hint: Use the .fit() method to train the model\n",
    "model_white.____(X_train_white, y_train_white)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "# Hint: Use the .predict() method on the model\n",
    "y_pred_white = model_white.____(X_test_white)\n",
    "# Hint: Use the accuracy_score function to compute the accuracy\n",
    "accuracy_white = accuracy_score(y_test_white, y_pred_white)\n",
    "print(f\"Accuracy: {accuracy_white:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Tuning and Optimization\n",
    "In this section, we will fine-tune the hyperparameters of our model to optimize its performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Grid Search\n",
    "\n",
    "We will use the '**GridSearchCV()**' function from the '**sklearn.model_selection**' module to perform a grid search for the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Retrain and Evaluate the Optimized Model\n",
    "\n",
    "Now, we will retrain our model using the best hyperparameters found during the grid search and evaluate its performance on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimized Random Forest Classifier\n",
    "optimized_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_optimized = optimized_model.predict(X_test)\n",
    "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "print(f\"Optimized Accuracy: {accuracy_optimized:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15:** Perform grid search for the best hyperparameters for the Random Forest Classifier on the white wine dataset, retrain the model using the best hyperparameters, and evaluate its performance using accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search for the best hyperparameters on the white wine dataset\n",
    "grid_search_white = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_white.fit(X_train_white, y_train_white)\n",
    "\n",
    "# Display the best hyperparameters found by the grid search\n",
    "# Hint: Use the .best_params_ attribute of the grid_search_white object\n",
    "best_params_white = grid_search_white.________\n",
    "print(f\"Best parameters: {best_params_white}\")\n",
    "\n",
    "# Train the optimized Random Forest Classifier using the best hyperparameters\n",
    "# Hint: Instantiate the RandomForestClassifier with the best_params_white and random_state parameter\n",
    "optimized_model_white = RandomForestClassifier(____=best_params_white, random_state=____)\n",
    "# Hint: Use the .fit() method to train the optimized model\n",
    "optimized_model_white.____(X_train_white, y_train_white)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "# Hint: Use the .predict() method on the optimized model\n",
    "y_pred_optimized_white = optimized_model_white.____(X_test_white)\n",
    "# Hint: Use the accuracy_score function to compute the optimized accuracy\n",
    "accuracy_optimized_white = accuracy_score(y_test_white, y_pred_optimized_white)\n",
    "print(f\"Optimized Accuracy: {accuracy_optimized_white:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.Conclusion\n",
    "In this tutorial, we covered various aspects of preparing data for machine learning algorithms and performing matrix manipulations using pandas. We started by loading and exploring the datasets, followed by data preprocessing, feature engineering, dataset splitting, and model selection. We also trained a Random Forest Classifier and optimized its hyperparameters using grid search. Through exercises, you got hands-on experience with data manipulation and learned various techniques that are essential for preparing data for machine learning algorithms.\n",
    "\n",
    "Remember that this tutorial is just a starting point, and there are numerous techniques and tools available to further enhance your skills in data preparation and manipulation. As you work with more datasets and different types of data, you will continue to gain experience and knowledge of various techniques that are essential for preparing data for machine learning algorithms.\n",
    "\n",
    "Now that you have completed the main material of the tutorial, you can continue to challenge yourself by working on these additional advanced exercises. These exercises will help you reinforce your understanding of the concepts and techniques covered in this tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Exercises\n",
    "**Exercise 1:** Perform exploratory data analysis on the white wine dataset, visualizing the distribution of the different features, and identify any outliers or potential issues with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to visualize the distribution of features in the white wine dataset\n",
    "import seaborn as sns\n",
    "\n",
    "# Replace this with the name of the column you want to visualize\n",
    "column_to_visualize = 'alcohol'\n",
    "\n",
    "# Hint: Use the sns.histplot function with the 'column_to_visualize' and kde=True\n",
    "sns.histplot(white_wine_df[_________], kde=____)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Normalize the features of the red wine dataset using Min-Max scaling, and compare the performance of the Random Forest Classifier on the original and normalized data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the features of the red wine dataset\n",
    "# Hint: Instantiate the MinMaxScaler\n",
    "scaler = _________()\n",
    "# Hint: Use the fit_transform method on the scaler for X_train\n",
    "X_train_scaled = scaler._____________(X_train)\n",
    "# Hint: Use the transform method on the scaler for X_test\n",
    "X_test_scaled = scaler._________(X_test)\n",
    "\n",
    "# Train the Random Forest Classifier on the normalized data\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_scaled = ___________________(random_state=42)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Scaled Accuracy: {accuracy_scaled:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Perform one-hot encoding on the categorical features created in the feature engineering section (e.g., 'sweetness' and 'acidity_level'), and retrain the Random Forest Classifier on the updated dataset. Compare the performance of the model with and without the one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the 'sweetness' and 'acidity_level' features\n",
    "# Hint: Use the pd.get_dummies function with the columns parameter for 'sweetness' and 'acidity_level'\n",
    "wine_df_encoded = pd.get_dummies(wine_df_with_sweetness, columns=[_____________])\n",
    "white_wine_df_encoded = pd.get_dummies(white_wine_df_with_acidity, columns=[_____________])\n",
    "\n",
    "# Train the Random Forest Classifier on the updated dataset with one-hot encoded features\n",
    "X_encoded = wine_df_encoded.drop(columns=['quality'])\n",
    "y_encoded = wine_df_encoded['quality']\n",
    "\n",
    "X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_encoded = ___________________(random_state=42)\n",
    "model_encoded.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_encoded = model_encoded.predict(X_test_encoded)\n",
    "accuracy_encoded = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(f\"Encoded Accuracy: {accuracy_encoded:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Experiment with different machine learning algorithms on the red wine and white wine datasets, such as Logistic Regression, Support Vector Machines, and K-Nearest Neighbors. Compare their performance with the Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the desired machine learning algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the selected model on the dataset\n",
    "# Hint: Instantiate the LogisticRegression with the random_state parameter\n",
    "new_model = ___________________(random_state=____)\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_new_model = new_model.predict(X_test)\n",
    "accuracy_new_model = accuracy_score(y_test, y_pred_new_model)\n",
    "print(f\"New Model Accuracy: {accuracy_new_model:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Use Recursive Feature Elimination (RFE) to select the most important features for the red wine and white wine datasets, and compare the performance of the machine learning models using the selected features versus the entire feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE object and compute a ranking of the features\n",
    "# Hint: Instantiate the RFE object with RandomForestClassifier and n_features_to_select\n",
    "selector = RFE(_____________________(random_state=42), n_features_to_select=5)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Select the most important features\n",
    "# Hint: Use the .support_ attribute of the selector object\n",
    "selected_features = X.columns[selector._______]\n",
    "\n",
    "# Train the Random Forest Classifier on the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_selected = ___________________(random_state=42)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_selected = model_selected.predict(X_test_selected)\n",
    "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
    "print(f\"Selected Features Accuracy: {accuracy_selected:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Perform Principal Component Analysis (PCA) on the red wine dataset to reduce its dimensionality and visualize the first two principal components. Train the Random Forest Classifier on the reduced dataset and compare its performance with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to the red wine dataset\n",
    "# Hint: Instantiate PCA with n_components=2\n",
    "pca = PCA(________________)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the Random Forest Classifier on the reduced dataset\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_pca = ___________________(random_state=42)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"PCA Accuracy: {accuracy_pca:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Using the red wine dataset, create a new feature that represents the ratio between residual sugar and alcohol. Train the Random Forest Classifier on the updated dataset and compare its performance with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature representing the ratio between residual sugar and alcohol\n",
    "# Hint: Divide the 'residual sugar' column by the 'alcohol' column\n",
    "wine_df_copy['sugar_alcohol_ratio'] = wine_df_copy[_________________] / wine_df_copy[_________________]\n",
    "\n",
    "# Train the Random Forest Classifier on the updated dataset\n",
    "X_ratio = wine_df_copy.drop(columns=['quality'])\n",
    "y_ratio = wine_df_copy['quality']\n",
    "\n",
    "X_train_ratio, X_test_ratio, y_train_ratio, y_test_ratio = train_test_split(X_ratio, y_ratio, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_ratio = ___________________(random_state=42)\n",
    "model_ratio.fit(X_train_ratio, y_train_ratio)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_ratio = model_ratio.predict(X_test_ratio)\n",
    "accuracy_ratio = accuracy_score(y_test_ratio, y_pred_ratio)\n",
    "print(f\"Ratio Feature Accuracy: {accuracy_ratio:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Perform k-means clustering on the red wine dataset and assign each sample to one of the clusters. Train the Random Forest Classifier on the dataset with the new cluster assignments as an additional feature and compare its performance with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform k-means clustering on the red wine dataset\n",
    "# Hint: Instantiate KMeans with n_clusters=3 and random_state=42\n",
    "kmeans = KMeans(___________________, random_state=42)\n",
    "wine_df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Train the Random Forest Classifier on the dataset with the new cluster assignments\n",
    "X_cluster = wine_df.drop(columns=['quality'])\n",
    "y_cluster = wine_df['quality']\n",
    "\n",
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(X_cluster, y_cluster, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_cluster = ___________________(random_state=42)\n",
    "model_cluster.fit(X_train_cluster, y_train_cluster)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_cluster = model_cluster.predict(X_test_cluster)\n",
    "accuracy_cluster = accuracy_score(y_test_cluster, y_pred_cluster)\n",
    "print(f\"Cluster Feature Accuracy: {accuracy_cluster:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Implement feature selection using a correlation matrix and a threshold for the correlation coefficient. Train the Random Forest Classifier on the dataset with the selected features and compare its performance with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for the red wine dataset\n",
    "corr_matrix = wine_df.corr()\n",
    "\n",
    "# Select features with a correlation coefficient below the threshold\n",
    "threshold = 0.6\n",
    "# Hint: Use a list comprehension to filter the columns\n",
    "selected_features = [column for column in corr_matrix.columns if abs(corr_matrix['quality'][column]) < threshold]\n",
    "\n",
    "# Train the Random Forest Classifier on the dataset with the selected features\n",
    "X_corr_selected = wine_df[selected_features]\n",
    "y_corr_selected = wine_df['quality']\n",
    "\n",
    "X_train_corr_selected, X_test_corr_selected, y_train_corr_selected, y_test_corr_selected = train_test_split(X_corr_selected, y_corr_selected, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_corr_selected = ___________________(random_state=42)\n",
    "model_corr_selected.fit(X_train_corr_selected, y_train_corr_selected)\n",
    "\n",
    "# Predict the target variable for the test set and compute the accuracy\n",
    "y_pred_corr_selected = model_corr_selected.predict(X_test_corr_selected)\n",
    "accuracy_corr_selected = accuracy_score(y_test_corr_selected, y_pred_corr_selected)\n",
    "print(f\"Correlation Selected Features Accuracy: {accuracy_corr_selected:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Implement a custom cross-validation strategy to evaluate the performance of the Random Forest Classifier on the red wine dataset. Compare the performance of the custom cross-validation strategy with the results obtained from the train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "# Implement custom cross-validation strategy\n",
    "def custom_cross_val_score(model, X, y, cv=5, random_state=42):\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Hint: Use the clone function from sklearn.base to make a copy of the model\n",
    "        model_cv = ___________(model)\n",
    "        model_cv.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = model_cv.predict(X_test_cv)\n",
    "        \n",
    "        score = accuracy_score(y_test_cv, y_pred_cv)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Evaluate the performance of the Random Forest Classifier using the custom cross-validation strategy\n",
    "# Hint: Instantiate the RandomForestClassifier with the random_state parameter\n",
    "model_cv = ___________________(random_state=42)\n",
    "custom_cv_score = custom_cross_val_score(model_cv, X, y, cv=5)\n",
    "print(f\"Custom Cross-Validation Score: {custom_cv_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
